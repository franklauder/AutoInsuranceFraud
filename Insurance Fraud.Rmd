---
title: "Insurance Fraud"
author: "Frank Laudert"
date: "2023-06-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

library(rmarkdown)

library(reticulate)



```



```{python}



import pandas as pd

import pickle

from pandas.api.types import is_numeric_dtype

import datetime

from datetime import date

from dateutil.relativedelta import relativedelta

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import plotly.express as px

import plotly.io as pio

import matplotlib.patches as mpatches

from plotnine import *

import plotnine

import scipy



```



# Introduction




Insurance fraud is a concern of many sectors such as health care, homeowners, and automobile. Insurance fraud is not only costly to insurers but also affects non fraudulent policy holders.

This analysis will focus on fraud within the auto insurance industry in India. The data used for this project was downloaded from Kaggle. <https://www.kaggle.com/>

Our goal is to use classification models for predicting which auto insurance claims are fraudulent. Several classification models will be assessed on their ability to successfully predict actual fraud.

The following programs were used for this project.

Python 3.10.10 <br/><br/> R 4.2.2 (Specific Visualizations) <br/><br/> RStudio 2023.03.1+446 (For document output)

<br/><br/>

<br/><br/>


# Exploratory Data Analysis


<br/><br/>

The data downloaded came in five data sets. We will review each data set for suitability of being merged into one data set.

<br/><br/>

```{python}


print('************Train_Claim_p Information************')

Train_Claim_p.info()







```




```{python}




print('************Train_Policy_p Information************')

Train_Policy_p.info()



```




```{python}


print('************Train_Demographics_p Information************')

Train_Demographics_p.info()



```




```{python}



print('**********Traindata_with_Targeet_p Information**********')

Traindata_with_Target_p.info()




```




```{python}


print('************Train_Vehicle_p Information************')

Train_Vehicle_p.info()


```





```{python}


print('*************Train_Vehicle_p First 25 Rows*************')

Train_Vehicle_p.head(25)



```




<br/><br/> <br/><br/>

The data sets train claim, train policy, train demographics, and train with target are ready to be merged into one data set.

Viewing the first twenty-five rows of the Train Vehicle data column VehicleAttribute we can see that it has multiple repeating rows as each customerID is as associated with Vehicle Model, Vehicle Make, Vehicle ID, and Vehicle YOM. The number of rows is 115344 which is four times the rows of the other data sets. This data set will have to be modified before it can be merged with the other data sets. Each level should be an individual feature matching to its corresponding level in the VehicleAtributeDetails feature. This will be accomplished by making the Train Vehicle data set wider. We will spread out the Vehicle Attribute feature so as each level will become a feature. This we make a new data set that is shorter and wider.

<br/><br/> <br/><br/>






```{python}


train_vehicle_wide=Train_Vehicle_p.pivot(index='CustomerID',columns='VehicleAttribute',values='VehicleAttributeDetails').reset_index()



```



```{python}


print('************train_vehicle_wide Information************')

train_vehicle_wide.info()



```




```{python}



print('*************train_vehicle_wide first 50 rows*************')


train_vehicle_wide.head(50)



```




<br/><br/>

We have taken the data from train vehicle and created a new data set called train vehicle wide. This new data set has four new columns and 28836 rows which now matches the other four data sets. We are now ready to merge all data sets.

<br/><br/>


```{python}

fraud=Train_Claim_p.merge(Train_Demographics_p, on="CustomerID")\
.merge(Train_Policy_p, on="CustomerID")\
.merge(train_vehicle_wide, on="CustomerID")\
.merge(Traindata_with_Target_p, on="CustomerID")



```



```{python}



print('*******************fraud Information*******************')

fraud.info()






```




## Feature Engineering/Cleaning


Feature engineering includes several steps.

First is feature creation. We create new variables from existing features which will help our model and data visualization.

Secondly, we can transform features from one representation to another. An example would be transforming a feature that is numerical to a type categorical.

Cleaning is the process of viewing the features and if something is not adding up with a feature, we can remove the values creating the problem or remove the feature entirely. An example is null values. We can replace a null value with another value, remove null values from the data set, or as mentioned before, remove the feature entirely.

<br/><br/>







```{python}

fraud_v2['DateOfIncident']=pd.to_datetime(fraud_v2['DateOfIncident'])



```



```{python}

fraud_v2['DateOfPolicyCoverage']=pd.to_datetime(fraud_v2['DateOfPolicyCoverage'])



```


```{python}


fraud_v2['dayOfWeek'] = fraud_v2["DateOfIncident"].dt.day_name()



```




```{python}




fraud_v2['dayOfWeek'].value_counts(normalize=True).round(2)




```





<br/><br/>

Certain features are numeric yet may better serve our models as categorical. This can be assessed by checking unique values of these features

<br/><br/>




```{python}


print(fraud_v2['BodilyInjuries'].unique())


```




<br/><br/>

The above outputs indicate that both NumberOfVehcicles and BodilyInjuries would be best as type categorical. We will create a function that converts numerical data types to categorical. Then the function will be applied to the selected numerical features.

<br/><br/>

create function



```{python}


def convert_to_cat(df, column_name):
  df[column_name]=df[column_name].astype('category')



```




```{python}


convert_to_cat(fraud_v2, 'NumberOfVehicles') 

convert_to_cat(fraud_v2, 'BodilyInjuries') 



```



```{python}


fraud_v2[['NumberOfVehicles', 'BodilyInjuries']].info()



```

<br/><br/>

Both features are now of type category

<br/><br/>


```{python}

print('*************Incident Time Unique Values*************')

print(fraud_v2['IncidentTime'].unique())



```


<br/><br/>

IncidentTime has unique values that would warrant it becoming categorical, though the many levels would not be optimal for use in our modeling. We can remedy this by placing unique time values into bins using a Python dictionary. This will reduce the number of levels.

<br/><br/>



```{python}


time_day={
  
  5:'early morning', 6:'early morning',7:'early morning',  8:'early morning',9:'late morning', 10: 'late morning', 11: 'late morning', 12:'early afternoon', 13:'early afternoon', 14:'early afternoon', 15:'early afternoon',16:'late afternoon', 17:'late afternoon', 18:'evening',
  19:'evening', 20:'night', 1:'night', 2:'night', 3:'night', 4:'night', 21:'night', 22:'night', 23:'night', 24:'night'
}



```




```{python}


fraud_v2['IncidentPeriodDay']=fraud_v2['IncidentTime'].map(time_day)



```






```{python}


print('***Incident Period Day Value Counts***')

print(fraud_v2['IncidentPeriodDay'].value_counts())



```




<br/><br/>

We find from the value count output for the new feature IncidentPeriodDay that incident times have been placed into six unique periods of the day.




<br/><br/>

Date features used in creatingnew features are no longer required and will be removed from the data set

<br/><br/>


```{python}



fraud_v3=fraud_v3.drop(['DateOfIncident', 'DateOfPolicyCoverage', 'IncidentTime'], axis=1)




```



<br/><br/>

For purposes of classification algorithms and visualizations we'll need to convert all categorical columns (Object Data Type) to the category data type. This will be accomplished by creating a function to identify non-numerical columns and converting them to the category data type.

<br/><br/>



```{python}

def convert_cats(df):
      cats = []
      for col in df.columns:
            if is_numeric_dtype(df[col]):
                  pass
            else:
                  cats.append(col)
      cat_indicies = []
      for col in cats:
            df[col] = df[col].astype('category')





```




```{python}

convert_cats(fraud_v3)




```



```{python}


print('*****************fraud_v3 Information*****************')

fraud_v3.info()



```




<br/><br/>

From the above output we observe that all object data types are now type categorical.

<br/><br/>




```{python}


count_plts_1=fraud_v3.filter(["TypeOfIncident", "TypeOfCollission","ReportedFraud"], axis=1)




```




```{python}


fig = plt.figure(figsize=(15, 10))
fig.suptitle('Categorical Counts-1', fontsize=18)
#sns.set(xlabel=None)
#sns.set(font_scale=0.5)
sns.set_style("dark")


plt.subplot(131)
plt.title('Type of Incident', fontsize=16)
ct_1=sns.countplot(data = count_plts_1, x = 'TypeOfIncident')
ct_1.tick_params(axis='x', which='major', labelsize=11)
ct_1.tick_params(axis='x', labelrotation=60 )
ct_1.set(xlabel=None) 



plt.subplot(132)
plt.title('Type of Collision', fontsize=16)
ct_2=sns.countplot(data = count_plts_1, x = 'TypeOfCollission')
ct_2.tick_params(axis='x', which='major', labelsize=11)
ct_2.tick_params(labelrotation=60)
ct_2.set(xlabel=None) 



plt.subplot(133)
plt.title('Reported Fraud', fontsize=16)
ct_3=sns.countplot(data = count_plts_1, x = 'ReportedFraud')
ct_3.tick_params(axis='x', which='major', labelsize=11)
#ct_3.tick_params(labelrotation=60)
ct_3.set(xlabel=None) 



plt.subplots_adjust(wspace=0.45)



```




```{python}



plt.show()
plt.clf()



```




```{python}

my_tab=pd.crosstab(index=fraud_v3["TypeOfIncident"], columns=fraud_v3["TypeOfCollission"], normalize=True).round(2)



```




```{python}

fig = plt.figure(figsize=(13, 13))

sns.heatmap(my_tab, cmap="BuGn",cbar=False, annot=True,linewidth=0.3)

plt.yticks(rotation=0)
plt.xticks(rotation=60)

plt.title('Type of Incident vs Type of Collision', fontsize=20)
plt.xlabel('TypeOfCollision', fontsize=15)
plt.ylabel('TypeOIncident', fontsize=15)



```




```{python}


plt.show()
plt.clf()


```



<br/><br/>

We observe from the cross table that the 'unknown' type of collision is only associated with a small number of incident types related to collisions. These data points will be retained by renaming the "unknown" column to "none".

<br/><br/>


```{python}


fraud_v4['TypeOfCollission'] = fraud_v4['TypeOfCollission'].replace(['?'], 'None')




```




```{python}


plt.figure(figsize=(16,10))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v4, x='TypeOfCollission')
#plt.tick_params(label_rotation=45)
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_title("Type of Collision-Changed", size=22)
ax.set(xlabel=None)
ax.set(ylabel=None)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()




```


```{python}


plt.show()
plt.clf()


```



```{python}


count_plts_2=fraud_v4.filter(['Witnesses', 'BodilyInjuries','PropertyDamage','NumberOfVehicles', 
'IncidentState', 'AuthoritiesContacted',"SeverityOfIncident"], axis=1)


```



```{python}



fig = plt.figure(figsize=(6, 6))
fig.tight_layout(pad=1.30,h_pad=4, w_pad=3)
fig.suptitle('Categorical Review-Two', fontsize=11)
sns.set_style("dark")





plt.subplot(331)
plt.title('Witnesses', fontsize=8, y=0.90)
dt_1=sns.countplot(data = count_plts_2, x = 'Witnesses')
dt_1.tick_params(axis='both', which='major', labelsize=4)
dt_1.tick_params(axis='x',labelrotation=35)
dt_1.set(xlabel=None) 
dt_1.set(ylabel=None) 





plt.subplot(332)
plt.title('Bodily Injuries',fontsize=8, y=0.90)
dt_2=sns.countplot(data = count_plts_2, x = 'BodilyInjuries')
dt_2.tick_params(axis='both', which='major', labelsize=6)
#dt_2.tick_params(axis='x',labelrotation=35)
dt_2.set(xlabel=None) 
dt_2.set(ylabel=None) 


plt.subplot(333)
plt.title('Property Damage',fontsize=8, y=0.90)
dt_3=sns.countplot(data = count_plts_2, x = 'PropertyDamage')
dt_3.tick_params(axis='both', which='major', labelsize=6)
#dt_3.tick_params(axis='x',labelrotation=35)
dt_3.set(xlabel=None) 
dt_3.set(ylabel=None) 

plt.subplot(334)
plt.title('Number Of Vehicles',fontsize=6, y=0.80)
dt_4=sns.countplot(data = count_plts_2, x = 'NumberOfVehicles')
dt_4.tick_params(axis='both', which='major', labelsize=6)
#dt_4.tick_params(axis='x',labelrotation=45)
dt_4.set(xlabel=None) 
dt_4.set(ylabel=None) 

plt.subplot(335)
plt.title('Incident State',fontsize=8, y=0.90)
dt_5=sns.countplot(data = count_plts_2, x = 'IncidentState')
dt_5.tick_params(axis='both', which='major', labelsize=6)
dt_5.tick_params(axis='x',labelrotation=90)
dt_5.set(xlabel=None) 
dt_5.set(ylabel=None) 

plt.subplot(336)
plt.title('Authorities Contacted',fontsize=8, y=0.90)
dt_6=sns.countplot(data = count_plts_2, x = 'AuthoritiesContacted')
dt_6.tick_params(axis='both', which='major', labelsize=6)
dt_6.tick_params(axis='x',labelrotation=90)
dt_6.set(xlabel=None) 
dt_6.set(ylabel=None) 

plt.subplot(337)
plt.title('SeverityOfIncident',fontsize=8, y=0.90)
dt_7=sns.countplot(data = count_plts_2, x ='SeverityOfIncident')
dt_7.tick_params(axis='both', which='major', labelsize=6)
dt_7.tick_params(axis='x',labelrotation=90)
dt_7.set(xlabel=None) 
dt_7.set(ylabel=None) 


plt.subplots_adjust(wspace=01.0, hspace=2.0)


```





```{python}


plt.show()
plt.clf()



```




From *figure 4* we detect certain features that must be dealt with due to missing values. First, the property damage feature will be dropped due to many observations having no answer which is denoted by a question mark.

<br/><br/>


```{python}



fraud_v5=fraud_v5.drop(['PropertyDamage'], axis=1)


```




<br/><br/>

Next, the category MISSINGVALUE from the Witnesses feature will be dropped.

<br/><br/>


```{python}



fraud_v5['Witnesses']=fraud_v5['Witnesses'].cat.remove_categories("MISSINGVALUE")



```



```{python}


plt.figure(figsize=(14,8))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v5, x='Witnesses')
#plt.tick_params(label_rotation=45)
ax.set_title("Witnesses-Changed", size=20)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='both', which='major', labelsize=14)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()


```







```{python}


count_plts_3=fraud_v5.filter(['PoliceReport', 'InsuredGender','InsuredEducationLevel', 'InsurancePolicyState','InsuredRelationship', 'dayOfWeek'], axis=1)



```



```{python}


fig = plt.figure(figsize=(10, 6))
fig.tight_layout(pad=1.40,h_pad=4, w_pad=3)
fig.suptitle('Categorical Review-Three', fontsize=13)
#sns.set(font_scale=0.5)
sns.set_style("dark")





plt.subplot(231)
plt.title('Police Report', fontsize=8, y=0.90)
et_1=sns.countplot(data = count_plts_3, x = 'PoliceReport')
et_1.tick_params(axis='both', which='major', labelsize=6)
et_1.tick_params(axis='x',labelrotation=75)
et_1.set(xlabel=None) 
et_1.set(ylabel=None) 






plt.subplot(232)
plt.title('Insured Gender',fontsize=8, y=0.90)
et_2=sns.countplot(data = count_plts_3, x = 'InsuredGender')
et_2.tick_params(axis='both', which='major', labelsize=6)
et_2.tick_params(axis='x',labelrotation=75)
et_2.set(xlabel=None) 
et_2.set(ylabel=None) 



plt.subplot(233)
plt.title('Insurance Policy State',fontsize=8, y=0.90)
et_4=sns.countplot(data = count_plts_3, x = 'InsurancePolicyState')
et_4.tick_params(axis='both', which='major', labelsize=6)
et_4.tick_params(axis='x',labelrotation=70)
et_4.set(xlabel=None) 

plt.subplot(234)
plt.title('Insured Education Level',fontsize=7, y=0.90)
et_3=sns.countplot(data = count_plts_3, x = 'InsuredEducationLevel')
et_3.tick_params(axis='both', which='major', labelsize=6)
et_3.tick_params(axis='x',labelrotation=90)
et_3.set(xlabel=None) 
et_3.set(ylabel=None) 

plt.subplot(235)
plt.title('Insured Relationship',fontsize=8, y=0.90)
et_5=sns.countplot(data = count_plts_3, x = 'InsuredRelationship')
et_5.tick_params(axis='both', which='major', labelsize=6)
et_5.tick_params(axis='x',labelrotation=90)
et_5.set(xlabel=None) 
et_5.set(ylabel=None) 



plt.subplot(236)
plt.title('Day of Week',fontsize=8, y=0.90)
et_6=sns.countplot(data = count_plts_3, x = 'dayOfWeek')
et_6.tick_params(axis='both', which='major', labelsize=6)
et_6.tick_params(axis='x',labelrotation=90)
et_6.set(xlabel=None) 
et_6.set(ylabel=None) 


plt.subplots_adjust(wspace=01.0, hspace=1.4)




```



```{python}



plt.show()
plt.clf()



```



```{python}



fraud_v5['Witnesses']=fraud_v5['Witnesses'].cat.remove_unused_categories()


```



Figure *Figure 6* informs us that there are additional categorical features which must be either cleaned or dropped. First, the feature Police Report has close to 10000 missing values (denoted by a question mark). This feature will be dropped.

<br/><br/>




```{python}


fraud_v6=fraud_v6.drop(['PoliceReport'], axis=1)



```




```{python}


The next feature requiring attention is InsuredGender. There are a small number of missing values, denoted by NA. This category will be removed from InsuredGender. The omission of this small count category will have no effect on our models.

<br/><br/>



```



```{python}


fraud_v6['InsuredGender']=fraud_v6['InsuredGender'].cat.remove_categories("NA")



```


```{python}


fraud_v6['InsuredGender']=fraud_v6['InsuredGender'].cat.remove_unused_categories()



```



```{python}


plt.figure(figsize=(14,10))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v6, x='InsuredGender')
#plt.tick_params(label_rotation=45)
ax.set_title("Insured Gender-Changed", size=25)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='both',labelsize = 15)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()




```




```{python}


plt.show()
plt.clf()



```





```{python}


premium_missing=fraud_v6[fraud_v6['PolicyAnnualPremium']==-1]



```



```{python}


#tick_params(labelrotation=70)


plt.figure(figsize=(16,6))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v6, x='VehicleMake')
#plt.tick_params(label_rotation=45)
ax.set_title("Vehicle Make", size=25)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='x',labelrotation=60,labelsize =13)
ax.tick_params(axis='y', labelsize=13)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()


```



```{python}

plt.show()
plt.clf()



```



<br/><br/>

VehicleMake has a small number of missing values (denoted by '???'). The category '???' will be removed from the feature.

<br/><br/>



```{python}


fraud_v7['VehicleMake']=fraud_v7['VehicleMake'].cat.remove_categories("???")


```




```{python}

fraud_v7['VehicleMake']=fraud_v7['VehicleMake'].cat.remove_unused_categories()



```




```{python}



vehicle_count['count']=1


```


```{python}



veh_mk=vehicle_count.groupby('VehicleMake')['count'].agg('count').reset_index()


```


```{python}


plt.figure(figsize=(16,10))

fig, axes=plt.subplots()

line_colors=['blue', 'cyan', 'green', 'red','skyblue','maroon', 'salmon', 'yellow', 
            'orange','lightgreen','darkviolet', 'fuchsia','darkmagenta','lime' ]
            
axes.hlines(veh_mk['VehicleMake'], xmin=0,
            xmax=veh_mk['count'],colors=line_colors)
            
axes.plot(veh_mk['count'],veh_mk['VehicleMake'],"o")
          
axes.set_xlim(0)

#plt.xlabel('Reported Fraude')
#plt.ylabel('Vehchle Make')
axes.tick_params(axis='both', which='major', labelsize=10)
plt.title('Make of Vehicle Count', fontsize=20)




```



```{python}


plt.show()
plt.clf()



```


<br/><br/>

*Figure 9* displays the VechicleMake feature with no missing values.

<br/><br/>

Filtering for any PolicyAnnualPremium value that is equal to -1 we find 141 values returned. From the Attribute Information pdf provided with the data set we know that -1 represents a missing value. All observations with -1 will be removed.

<br/><br/>

<br/><br/>




```{python}

fraud_v7=fraud_v7[fraud_v7['PolicyAnnualPremium']!=-1]


```




```{python}


print('**Policy Annual Premium Shape**')

fraud_v7[fraud_v7['PolicyAnnualPremium']==-1].shape


```

<br/><br/>

From the size output we can observe all values of -1 have been removed.

<br/><br/>


```{python}



count_plts_4=fraud_v7.filter(['InsuredOccupation', 'InsuredHobbies','VehicleMake'], axis=1)


```

<br/><br/>

Certain visualizations require numeric only data. We'll create a date set that contains only numeric data types.

<br/><br/>

```{python}

#select only the numeric columns in the DataFrame

numeric_data=fraud_v7.select_dtypes(include=np.number)



```



```{python}



numeric_data=numeric_data.drop(['InsuredZipCode', 'InsurancePolicyNumber'], axis=1)


```



```{python}


#display data type of each variable in DataFrame


print("******************Numeric Data Types******************")

print(numeric_data.dtypes)


```

The data set numeric_data only has features of numeric data types as seen from the above output.

<br/><br/>

```{python}


plt.figure(figsize=(10, 7))

#sns.set(font_scale=3)


plt.tick_params(axis='both', which='major', labelsize=9)



plt.title('Correlation Heatmap', fontsize=12)

# define the mask to set the values in the upper triangle to Truemask 

mask=np.triu(np.ones_like(numeric_data.corr(), dtype=bool))

# Generate a custom diverging colormap

#cmap = sns.diverging_palette(220, 10, as_cmap=True)

#ht_mp=sns.heatmap(fraud_train_v8.corr(), cmap=cmap, vmax=.3, center=0,annot=True,
            #square=True, linewidths=.5, cbar_kws={"shrink": .5})
            
            
heatmap = sns.heatmap(numeric_data.corr(), mask=mask,vmin=-1, vmax=1, annot=True, cmap='BrBG', annot_kws={"size": 4})





            
            
#heatmap.set_title('Correlation Heatmap', #fontdict={'fontsize':20}, pad=12)
            



```



```{python}



plt.show()

plt.clf()



```


<br/><br/> <br/><br/>

There is very high to high correlation between Amount of Injury Claim, Amount of Property Claim, Amount of Vehicle Damage, and Amount of Total Claim. This is unsurprising as Amount of Total Claim is the sum of the other three. Amount of Total Claim is the only feature of the four that will be used for our machine learning models.

Other features exhibiting very high correlation are Loyalty period and Age. This makes sense as older customers have the chance to accrue loyalty time based on having lived longer than younger customers. Still, we will retain both features for our models.

<br/><br/>



Features not important for visualizing or building models will be dropped.

<br/><br/>

```{python}


fraud_v8=fraud_v8.drop(['CustomerID', 'IncidentAddress', 'InsuredZipCode', 'InsuredHobbies','Country', 'InsurancePolicyNumber', 'VehicleID'], axis=1)


```



```{python}



fraud_v8=fraud_v8.dropna()



```



<br/><br/>

<br/><br/>

## Visualization

<br/><br/>

<br/><br/>


```{python}






```



```{python}





```



```{python}






```



```{python}






```









