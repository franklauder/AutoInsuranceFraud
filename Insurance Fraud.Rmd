---
title: "Insurance Fraud"
author: "Frank Laudert"
date: "2023-06-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}

library(rmarkdown)

library(reticulate)



```



```{python}



import pandas as pd

import pickle

from pandas.api.types import is_numeric_dtype

import datetime

from datetime import date

from dateutil.relativedelta import relativedelta

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import plotly.express as px

import plotly.io as pio

import matplotlib.patches as mpatches

from plotnine import *

import plotnine

import scipy



```



# Introduction




Insurance fraud is a concern of many sectors such as health care, homeowners, and automobile. Insurance fraud is not only costly to insurers but also affects non fraudulent policy holders.

This analysis will focus on fraud within the auto insurance industry in India. The data used for this project was downloaded from Kaggle. <https://www.kaggle.com/>

Our goal is to use classification models for predicting which auto insurance claims are fraudulent. Several classification models will be assessed on their ability to successfully predict actual fraud.

The following programs were used for this project.

Python 3.10.10 <br/><br/> R 4.2.2 (Specific Visualizations) <br/><br/> RStudio 2023.03.1+446 (For document output)

<br/><br/>

<br/><br/>


# Exploratory Data Analysis


<br/><br/>

The data downloaded came in five data sets. We will review each data set for suitability of being merged into one data set.

<br/><br/>

```{python}


print('************Train_Claim_p Information************')

Train_Claim_p.info()







```




```{python}




print('************Train_Policy_p Information************')

Train_Policy_p.info()



```




```{python}


print('************Train_Demographics_p Information************')

Train_Demographics_p.info()



```




```{python}



print('**********Traindata_with_Targeet_p Information**********')

Traindata_with_Target_p.info()




```




```{python}


print('************Train_Vehicle_p Information************')

Train_Vehicle_p.info()


```





```{python}


print('*************Train_Vehicle_p First 25 Rows*************')

Train_Vehicle_p.head(25)



```




<br/><br/> <br/><br/>

The data sets train claim, train policy, train demographics, and train with target are ready to be merged into one data set.

Viewing the first twenty-five rows of the Train Vehicle data column VehicleAttribute we can see that it has multiple repeating rows as each customerID is as associated with Vehicle Model, Vehicle Make, Vehicle ID, and Vehicle YOM. The number of rows is 115344 which is four times the rows of the other data sets. This data set will have to be modified before it can be merged with the other data sets. Each level should be an individual feature matching to its corresponding level in the VehicleAtributeDetails feature. This will be accomplished by making the Train Vehicle data set wider. We will spread out the Vehicle Attribute feature so as each level will become a feature. This we make a new data set that is shorter and wider.

<br/><br/> <br/><br/>






```{python}


train_vehicle_wide=Train_Vehicle_p.pivot(index='CustomerID',columns='VehicleAttribute',values='VehicleAttributeDetails').reset_index()



```



```{python}


print('************train_vehicle_wide Information************')

train_vehicle_wide.info()



```




```{python}



print('*************train_vehicle_wide first 50 rows*************')


train_vehicle_wide.head(50)



```




<br/><br/>

We have taken the data from train vehicle and created a new data set called train vehicle wide. This new data set has four new columns and 28836 rows which now matches the other four data sets. We are now ready to merge all data sets.

<br/><br/>


```{python}

fraud=Train_Claim_p.merge(Train_Demographics_p, on="CustomerID")\
.merge(Train_Policy_p, on="CustomerID")\
.merge(train_vehicle_wide, on="CustomerID")\
.merge(Traindata_with_Target_p, on="CustomerID")



```



```{python}



print('*******************fraud Information*******************')

fraud.info()






```




## Feature Engineering/Cleaning


Feature engineering includes several steps.

First is feature creation. We create new variables from existing features which will help our model and data visualization.

Secondly, we can transform features from one representation to another. An example would be transforming a feature that is numerical to a type categorical.

Cleaning is the process of viewing the features and if something is not adding up with a feature, we can remove the values creating the problem or remove the feature entirely. An example is null values. We can replace a null value with another value, remove null values from the data set, or as mentioned before, remove the feature entirely.

<br/><br/>







```{python}

fraud_v2['DateOfIncident']=pd.to_datetime(fraud_v2['DateOfIncident'])



```



```{python}

fraud_v2['DateOfPolicyCoverage']=pd.to_datetime(fraud_v2['DateOfPolicyCoverage'])



```


```{python}


fraud_v2['dayOfWeek'] = fraud_v2["DateOfIncident"].dt.day_name()



```




```{python}




fraud_v2['dayOfWeek'].value_counts(normalize=True).round(2)




```





<br/><br/>

Certain features are numeric yet may better serve our models as categorical. This can be assessed by checking unique values of these features

<br/><br/>




```{python}


print(fraud_v2['BodilyInjuries'].unique())


```




<br/><br/>

The above outputs indicate that both NumberOfVehcicles and BodilyInjuries would be best as type categorical. We will create a function that converts numerical data types to categorical. Then the function will be applied to the selected numerical features.

<br/><br/>

create function



```{python}


def convert_to_cat(df, column_name):
  df[column_name]=df[column_name].astype('category')



```




```{python}


convert_to_cat(fraud_v2, 'NumberOfVehicles') 

convert_to_cat(fraud_v2, 'BodilyInjuries') 



```



```{python}


fraud_v2[['NumberOfVehicles', 'BodilyInjuries']].info()



```

<br/><br/>

Both features are now of type category

<br/><br/>


```{python}

print('*************Incident Time Unique Values*************')

print(fraud_v2['IncidentTime'].unique())



```


<br/><br/>

IncidentTime has unique values that would warrant it becoming categorical, though the many levels would not be optimal for use in our modeling. We can remedy this by placing unique time values into bins using a Python dictionary. This will reduce the number of levels.

<br/><br/>



```{python}


time_day={
  
  5:'early morning', 6:'early morning',7:'early morning',  8:'early morning',9:'late morning', 10: 'late morning', 11: 'late morning', 12:'early afternoon', 13:'early afternoon', 14:'early afternoon', 15:'early afternoon',16:'late afternoon', 17:'late afternoon', 18:'evening',
  19:'evening', 20:'night', 1:'night', 2:'night', 3:'night', 4:'night', 21:'night', 22:'night', 23:'night', 24:'night'
}



```




```{python}


fraud_v2['IncidentPeriodDay']=fraud_v2['IncidentTime'].map(time_day)



```






```{python}


print('***Incident Period Day Value Counts***')

print(fraud_v2['IncidentPeriodDay'].value_counts())



```




<br/><br/>

We find from the value count output for the new feature IncidentPeriodDay that incident times have been placed into six unique periods of the day.




<br/><br/>

Date features used in creatingnew features are no longer required and will be removed from the data set

<br/><br/>


```{python}



fraud_v3=fraud_v3.drop(['DateOfIncident', 'DateOfPolicyCoverage', 'IncidentTime'], axis=1)




```



<br/><br/>

For purposes of classification algorithms and visualizations we'll need to convert all categorical columns (Object Data Type) to the category data type. This will be accomplished by creating a function to identify non-numerical columns and converting them to the category data type.

<br/><br/>



```{python}

def convert_cats(df):
      cats = []
      for col in df.columns:
            if is_numeric_dtype(df[col]):
                  pass
            else:
                  cats.append(col)
      cat_indicies = []
      for col in cats:
            df[col] = df[col].astype('category')





```




```{python}

convert_cats(fraud_v3)




```



```{python}


print('*****************fraud_v3 Information*****************')

fraud_v3.info()



```




<br/><br/>

From the above output we observe that all object data types are now type categorical.

<br/><br/>




```{python}


count_plts_1=fraud_v3.filter(["TypeOfIncident", "TypeOfCollission","ReportedFraud"], axis=1)




```




```{python}


fig = plt.figure(figsize=(15, 10))
fig.suptitle('Categorical Counts-1', fontsize=18)
#sns.set(xlabel=None)
#sns.set(font_scale=0.5)
sns.set_style("dark")


plt.subplot(131)
plt.title('Type of Incident', fontsize=16)
ct_1=sns.countplot(data = count_plts_1, x = 'TypeOfIncident')
ct_1.tick_params(axis='x', which='major', labelsize=11)
ct_1.tick_params(axis='x', labelrotation=60 )
ct_1.set(xlabel=None) 



plt.subplot(132)
plt.title('Type of Collision', fontsize=16)
ct_2=sns.countplot(data = count_plts_1, x = 'TypeOfCollission')
ct_2.tick_params(axis='x', which='major', labelsize=11)
ct_2.tick_params(labelrotation=60)
ct_2.set(xlabel=None) 



plt.subplot(133)
plt.title('Reported Fraud', fontsize=16)
ct_3=sns.countplot(data = count_plts_1, x = 'ReportedFraud')
ct_3.tick_params(axis='x', which='major', labelsize=11)
#ct_3.tick_params(labelrotation=60)
ct_3.set(xlabel=None) 



plt.subplots_adjust(wspace=0.45)



```




```{python}



plt.show()
plt.clf()



```




```{python}

my_tab=pd.crosstab(index=fraud_v3["TypeOfIncident"], columns=fraud_v3["TypeOfCollission"], normalize=True).round(2)



```




```{python}

fig = plt.figure(figsize=(13, 13))

sns.heatmap(my_tab, cmap="BuGn",cbar=False, annot=True,linewidth=0.3)

plt.yticks(rotation=0)
plt.xticks(rotation=60)

plt.title('Type of Incident vs Type of Collision', fontsize=20)
plt.xlabel('TypeOfCollision', fontsize=15)
plt.ylabel('TypeOIncident', fontsize=15)



```




```{python}


plt.show()
plt.clf()


```



<br/><br/>

We observe from the cross table that the 'unknown' type of collision is only associated with a small number of incident types related to collisions. These data points will be retained by renaming the "unknown" column to "none".

<br/><br/>


```{python}


fraud_v4['TypeOfCollission'] = fraud_v4['TypeOfCollission'].replace(['?'], 'None')




```




```{python}


plt.figure(figsize=(16,10))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v4, x='TypeOfCollission')
#plt.tick_params(label_rotation=45)
ax.tick_params(axis='both', which='major', labelsize=11)
ax.set_title("Type of Collision-Changed", size=22)
ax.set(xlabel=None)
ax.set(ylabel=None)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()




```


```{python}


plt.show()
plt.clf()


```



```{python}


count_plts_2=fraud_v4.filter(['Witnesses', 'BodilyInjuries','PropertyDamage','NumberOfVehicles', 
'IncidentState', 'AuthoritiesContacted',"SeverityOfIncident"], axis=1)


```



```{python}



fig = plt.figure(figsize=(6, 6))
fig.tight_layout(pad=1.30,h_pad=4, w_pad=3)
fig.suptitle('Categorical Review-Two', fontsize=11)
sns.set_style("dark")





plt.subplot(331)
plt.title('Witnesses', fontsize=8, y=0.90)
dt_1=sns.countplot(data = count_plts_2, x = 'Witnesses')
dt_1.tick_params(axis='both', which='major', labelsize=4)
dt_1.tick_params(axis='x',labelrotation=35)
dt_1.set(xlabel=None) 
dt_1.set(ylabel=None) 





plt.subplot(332)
plt.title('Bodily Injuries',fontsize=8, y=0.90)
dt_2=sns.countplot(data = count_plts_2, x = 'BodilyInjuries')
dt_2.tick_params(axis='both', which='major', labelsize=6)
#dt_2.tick_params(axis='x',labelrotation=35)
dt_2.set(xlabel=None) 
dt_2.set(ylabel=None) 


plt.subplot(333)
plt.title('Property Damage',fontsize=8, y=0.90)
dt_3=sns.countplot(data = count_plts_2, x = 'PropertyDamage')
dt_3.tick_params(axis='both', which='major', labelsize=6)
#dt_3.tick_params(axis='x',labelrotation=35)
dt_3.set(xlabel=None) 
dt_3.set(ylabel=None) 

plt.subplot(334)
plt.title('Number Of Vehicles',fontsize=6, y=0.80)
dt_4=sns.countplot(data = count_plts_2, x = 'NumberOfVehicles')
dt_4.tick_params(axis='both', which='major', labelsize=6)
#dt_4.tick_params(axis='x',labelrotation=45)
dt_4.set(xlabel=None) 
dt_4.set(ylabel=None) 

plt.subplot(335)
plt.title('Incident State',fontsize=8, y=0.90)
dt_5=sns.countplot(data = count_plts_2, x = 'IncidentState')
dt_5.tick_params(axis='both', which='major', labelsize=6)
dt_5.tick_params(axis='x',labelrotation=90)
dt_5.set(xlabel=None) 
dt_5.set(ylabel=None) 

plt.subplot(336)
plt.title('Authorities Contacted',fontsize=8, y=0.90)
dt_6=sns.countplot(data = count_plts_2, x = 'AuthoritiesContacted')
dt_6.tick_params(axis='both', which='major', labelsize=6)
dt_6.tick_params(axis='x',labelrotation=90)
dt_6.set(xlabel=None) 
dt_6.set(ylabel=None) 

plt.subplot(337)
plt.title('SeverityOfIncident',fontsize=8, y=0.90)
dt_7=sns.countplot(data = count_plts_2, x ='SeverityOfIncident')
dt_7.tick_params(axis='both', which='major', labelsize=6)
dt_7.tick_params(axis='x',labelrotation=90)
dt_7.set(xlabel=None) 
dt_7.set(ylabel=None) 


plt.subplots_adjust(wspace=01.0, hspace=2.0)


```





```{python}


plt.show()
plt.clf()



```




From *figure 4* we detect certain features that must be dealt with due to missing values. First, the property damage feature will be dropped due to many observations having no answer which is denoted by a question mark.

<br/><br/>


```{python}



fraud_v5=fraud_v5.drop(['PropertyDamage'], axis=1)


```




<br/><br/>

Next, the category MISSINGVALUE from the Witnesses feature will be dropped.

<br/><br/>


```{python}



fraud_v5['Witnesses']=fraud_v5['Witnesses'].cat.remove_categories("MISSINGVALUE")



```



```{python}


plt.figure(figsize=(14,8))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v5, x='Witnesses')
#plt.tick_params(label_rotation=45)
ax.set_title("Witnesses-Changed", size=20)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='both', which='major', labelsize=14)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()


```







```{python}


count_plts_3=fraud_v5.filter(['PoliceReport', 'InsuredGender','InsuredEducationLevel', 'InsurancePolicyState','InsuredRelationship', 'dayOfWeek'], axis=1)



```



```{python}


fig = plt.figure(figsize=(10, 6))
fig.tight_layout(pad=1.40,h_pad=4, w_pad=3)
fig.suptitle('Categorical Review-Three', fontsize=13)
#sns.set(font_scale=0.5)
sns.set_style("dark")





plt.subplot(231)
plt.title('Police Report', fontsize=8, y=0.90)
et_1=sns.countplot(data = count_plts_3, x = 'PoliceReport')
et_1.tick_params(axis='both', which='major', labelsize=6)
et_1.tick_params(axis='x',labelrotation=75)
et_1.set(xlabel=None) 
et_1.set(ylabel=None) 






plt.subplot(232)
plt.title('Insured Gender',fontsize=8, y=0.90)
et_2=sns.countplot(data = count_plts_3, x = 'InsuredGender')
et_2.tick_params(axis='both', which='major', labelsize=6)
et_2.tick_params(axis='x',labelrotation=75)
et_2.set(xlabel=None) 
et_2.set(ylabel=None) 



plt.subplot(233)
plt.title('Insurance Policy State',fontsize=8, y=0.90)
et_4=sns.countplot(data = count_plts_3, x = 'InsurancePolicyState')
et_4.tick_params(axis='both', which='major', labelsize=6)
et_4.tick_params(axis='x',labelrotation=70)
et_4.set(xlabel=None) 

plt.subplot(234)
plt.title('Insured Education Level',fontsize=7, y=0.90)
et_3=sns.countplot(data = count_plts_3, x = 'InsuredEducationLevel')
et_3.tick_params(axis='both', which='major', labelsize=6)
et_3.tick_params(axis='x',labelrotation=90)
et_3.set(xlabel=None) 
et_3.set(ylabel=None) 

plt.subplot(235)
plt.title('Insured Relationship',fontsize=8, y=0.90)
et_5=sns.countplot(data = count_plts_3, x = 'InsuredRelationship')
et_5.tick_params(axis='both', which='major', labelsize=6)
et_5.tick_params(axis='x',labelrotation=90)
et_5.set(xlabel=None) 
et_5.set(ylabel=None) 



plt.subplot(236)
plt.title('Day of Week',fontsize=8, y=0.90)
et_6=sns.countplot(data = count_plts_3, x = 'dayOfWeek')
et_6.tick_params(axis='both', which='major', labelsize=6)
et_6.tick_params(axis='x',labelrotation=90)
et_6.set(xlabel=None) 
et_6.set(ylabel=None) 


plt.subplots_adjust(wspace=01.0, hspace=1.4)




```



```{python}



plt.show()
plt.clf()



```



```{python}



fraud_v5['Witnesses']=fraud_v5['Witnesses'].cat.remove_unused_categories()


```



Figure *Figure 6* informs us that there are additional categorical features which must be either cleaned or dropped. First, the feature Police Report has close to 10000 missing values (denoted by a question mark). This feature will be dropped.

<br/><br/>




```{python}


fraud_v6=fraud_v6.drop(['PoliceReport'], axis=1)



```




```{python}


The next feature requiring attention is InsuredGender. There are a small number of missing values, denoted by NA. This category will be removed from InsuredGender. The omission of this small count category will have no effect on our models.

<br/><br/>



```



```{python}


fraud_v6['InsuredGender']=fraud_v6['InsuredGender'].cat.remove_categories("NA")



```


```{python}


fraud_v6['InsuredGender']=fraud_v6['InsuredGender'].cat.remove_unused_categories()



```



```{python}


plt.figure(figsize=(14,10))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v6, x='InsuredGender')
#plt.tick_params(label_rotation=45)
ax.set_title("Insured Gender-Changed", size=25)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='both',labelsize = 15)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()




```




```{python}


plt.show()
plt.clf()



```





```{python}


premium_missing=fraud_v6[fraud_v6['PolicyAnnualPremium']==-1]



```



```{python}


#tick_params(labelrotation=70)


plt.figure(figsize=(16,6))
#plt.title("Type of Collision-Changed")
ax=sns.countplot(data=fraud_v6, x='VehicleMake')
#plt.tick_params(label_rotation=45)
ax.set_title("Vehicle Make", size=25)
ax.set(xlabel=None)
ax.set(ylabel=None)
ax.tick_params(axis='x',labelrotation=60,labelsize =13)
ax.tick_params(axis='y', labelsize=13)
sns.set_style("dark")

ax.annotate('Figure ##',

            xy = (1.0, -0.2),

            xycoords='axes fraction',

            ha='right',

            va="center",

            fontsize=10)
            
fig.tight_layout()


```



```{python}

plt.show()
plt.clf()



```



<br/><br/>

VehicleMake has a small number of missing values (denoted by '???'). The category '???' will be removed from the feature.

<br/><br/>



```{python}


fraud_v7['VehicleMake']=fraud_v7['VehicleMake'].cat.remove_categories("???")


```




```{python}

fraud_v7['VehicleMake']=fraud_v7['VehicleMake'].cat.remove_unused_categories()



```




```{python}



vehicle_count['count']=1


```


```{python}



veh_mk=vehicle_count.groupby('VehicleMake')['count'].agg('count').reset_index()


```


```{python}


plt.figure(figsize=(16,10))

fig, axes=plt.subplots()

line_colors=['blue', 'cyan', 'green', 'red','skyblue','maroon', 'salmon', 'yellow', 
            'orange','lightgreen','darkviolet', 'fuchsia','darkmagenta','lime' ]
            
axes.hlines(veh_mk['VehicleMake'], xmin=0,
            xmax=veh_mk['count'],colors=line_colors)
            
axes.plot(veh_mk['count'],veh_mk['VehicleMake'],"o")
          
axes.set_xlim(0)

#plt.xlabel('Reported Fraude')
#plt.ylabel('Vehchle Make')
axes.tick_params(axis='both', which='major', labelsize=10)
plt.title('Make of Vehicle Count', fontsize=20)




```



```{python}


plt.show()
plt.clf()



```


<br/><br/>

*Figure 9* displays the VechicleMake feature with no missing values.

<br/><br/>

Filtering for any PolicyAnnualPremium value that is equal to -1 we find 141 values returned. From the Attribute Information pdf provided with the data set we know that -1 represents a missing value. All observations with -1 will be removed.

<br/><br/>

<br/><br/>




```{python}

fraud_v7=fraud_v7[fraud_v7['PolicyAnnualPremium']!=-1]


```




```{python}


print('**Policy Annual Premium Shape**')

fraud_v7[fraud_v7['PolicyAnnualPremium']==-1].shape


```

<br/><br/>

From the size output we can observe all values of -1 have been removed.

<br/><br/>


```{python}



count_plts_4=fraud_v7.filter(['InsuredOccupation', 'InsuredHobbies','VehicleMake'], axis=1)


```

<br/><br/>

Certain visualizations require numeric only data. We'll create a date set that contains only numeric data types.

<br/><br/>

```{python}

#select only the numeric columns in the DataFrame

numeric_data=fraud_v7.select_dtypes(include=np.number)



```



```{python}



numeric_data=numeric_data.drop(['InsuredZipCode', 'InsurancePolicyNumber'], axis=1)


```



```{python}


#display data type of each variable in DataFrame


print("******************Numeric Data Types******************")

print(numeric_data.dtypes)


```

The data set numeric_data only has features of numeric data types as seen from the above output.

<br/><br/>

```{python}


plt.figure(figsize=(10, 7))

#sns.set(font_scale=3)


plt.tick_params(axis='both', which='major', labelsize=9)



plt.title('Correlation Heatmap', fontsize=12)

# define the mask to set the values in the upper triangle to Truemask 

mask=np.triu(np.ones_like(numeric_data.corr(), dtype=bool))

# Generate a custom diverging colormap

#cmap = sns.diverging_palette(220, 10, as_cmap=True)

#ht_mp=sns.heatmap(fraud_train_v8.corr(), cmap=cmap, vmax=.3, center=0,annot=True,
            #square=True, linewidths=.5, cbar_kws={"shrink": .5})
            
            
heatmap = sns.heatmap(numeric_data.corr(), mask=mask,vmin=-1, vmax=1, annot=True, cmap='BrBG', annot_kws={"size": 4})





            
            
#heatmap.set_title('Correlation Heatmap', #fontdict={'fontsize':20}, pad=12)
            



```



```{python}



plt.show()

plt.clf()



```


<br/><br/> <br/><br/>

There is very high to high correlation between Amount of Injury Claim, Amount of Property Claim, Amount of Vehicle Damage, and Amount of Total Claim. This is unsurprising as Amount of Total Claim is the sum of the other three. Amount of Total Claim is the only feature of the four that will be used for our machine learning models.

Other features exhibiting very high correlation are Loyalty period and Age. This makes sense as older customers have the chance to accrue loyalty time based on having lived longer than younger customers. Still, we will retain both features for our models.

<br/><br/>



Features not important for visualizing or building models will be dropped.

<br/><br/>

```{python}


fraud_v8=fraud_v8.drop(['CustomerID', 'IncidentAddress', 'InsuredZipCode', 'InsuredHobbies','Country', 'InsurancePolicyNumber', 'VehicleID'], axis=1)


```



```{python}



fraud_v8=fraud_v8.dropna()



```



<br/><br/>

<br/><br/>

## Visualization

<br/><br/>

<br/><br/>

```{python}




fig = plt.figure(figsize=(10, 7))
fig.tight_layout(pad=1.30,h_pad=4, w_pad=3)
fig.suptitle('Numerical Review by Fraud Reported', fontsize=11)
sns.set_style("dark")





plt.subplot(331)
plt.title('Amount of Total Claim', fontsize=7, y=0.90)
dt_1=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="AmountOfTotalClaim")
dt_1.tick_params(axis='both', which='major', labelsize=4)
#dt_1.tick_params(axis='x',labelrotation=90)
dt_1.set(xlabel=None) 
dt_1.set(ylabel=None) 





plt.subplot(332)
plt.title("Insured Age",fontsize=7, y=0.90)
dt_2=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="InsuredAge")
dt_2.tick_params(axis='both', which='major', labelsize=6)
#dt_2.tick_params(axis='x',labelrotation=90)
dt_2.set(xlabel=None) 
dt_2.set(ylabel=None) 


plt.subplot(333)
plt.title("Capital Gains",fontsize=7, y=0.90)
dt_3=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="CapitalGains")
dt_3.tick_params(axis='both', which='major', labelsize=6)
#dt_3.tick_params(axis='x',labelrotation=90)
dt_3.set(xlabel=None) 
dt_3.set(ylabel=None) 

plt.subplot(334)
plt.title("Capital Loss",fontsize=7, y=0.80)
dt_4=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="CapitalLoss")
dt_4.tick_params(axis='both', which='major', labelsize=6)
#dt_4.tick_params(axis='x',labelrotation=45)
dt_4.set(xlabel=None) 
dt_4.set(ylabel=None) 

plt.subplot(335)
plt.title("Customer Loyalty Period",fontsize=7, y=0.90)
dt_5=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="CustomerLoyaltyPeriod")
dt_5.tick_params(axis='both', which='major', labelsize=6)
#dt_5.tick_params(axis='x',labelrotation=90)
dt_5.set(xlabel=None) 
dt_5.set(ylabel=None) 

plt.subplot(336)
plt.title("Policy Deductible",fontsize=7, y=0.90)
dt_6=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="Policy_Deductible")
dt_6.tick_params(axis='both', which='major', labelsize=6)
#dt_6.tick_params(axis='x',labelrotation=90)
dt_6.set(xlabel=None) 
dt_6.set(ylabel=None) 

plt.subplot(337)
plt.title("Policy Annual Premium",fontsize=7, y=0.90)
dt_7=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="PolicyAnnualPremium")
dt_7.tick_params(axis='both', which='major', labelsize=6)
#dt_7.tick_params(axis='x',labelrotation=90)
dt_7.set(xlabel=None) 
dt_7.set(ylabel=None) 

plt.subplot(338)
plt.title("Umbrella Limit",fontsize=7, y=0.90)
dt_8=sns.boxplot(data = fraud_v8, x = 'ReportedFraud', y="UmbrellaLimit")
dt_8.tick_params(axis='both', which='major', labelsize=6)
#dt_8.tick_params(axis='x',labelrotation=90)
dt_8.set(xlabel=None) 
dt_8.set(ylabel=None) 




plt.subplots_adjust(wspace=01.0, hspace=2.0)


txt="Figure 3. Values collected by user4545 are plotted."
plt.figtext(0.5, -0.21, txt, wrap=True, horizontalalignment='center', fontsize=5);




```



```{python}





```



Based on the subplots from *Figure 11*, we observe that certain numeric features have outliers. We'll take a closer look at those features.

<br/><br/>

```{python}



fig = plt.figure(figsize=(11, 6))
fig.suptitle('Amount of Total Claim', fontsize=11)
#sns.set(xlabel=None)
#sns.set(font_scale=0.5)
sns.set_style("dark")






plt.subplot(131)
plt.title('Box Plot-Type of Incident and Reported Fraud', fontsize=7)
ac_1=sns.boxplot(data = fraud_v8, x = "AmountOfTotalClaim", y='ReportedFraud')
ac_1.tick_params(axis='x', which='major', labelsize=5)
ac_1.tick_params(axis='y', labelsize=5)
ac_1.tick_params(axis='x', labelrotation=60)
ac_1.set(xlabel=None) 



plt.subplot(132)
plt.title('Histogram-Amount of Total Claim', fontsize=7)
ac_2=sns.histplot(data=fraud_v8, x="AmountOfTotalClaim")
ac_2.tick_params(axis='x', which='major', labelsize=5)
ac_2.tick_params(labelrotation=60)
ac_2.tick_params(axis='y', labelsize=5)
ac_2.set(xlabel=None) 



plt.subplot(133)
plt.title('Histogram-Amount of Total Claim and Reported Fraud', fontsize=7)
ac_3=sns.histplot(data=fraud_v8, x="AmountOfTotalClaim", hue="ReportedFraud")
ac_3.tick_params(axis='x', which='major', labelsize=5)
ac_3.tick_params(axis='y', labelsize=5)
#ac_3.tick_params(labelrotation=60)
ac_3.set(xlabel=None) 



plt.subplots_adjust(wspace=0.45)




```


```{python}






```

The box plots from *Figure 13* displaying Amount of Total Claim for the different events of ReportedFraud are interesting. For Reported Fraud=Y there are many outliers which are below 22000. Data points falling under 22000 for ReportedFraud='N" is not considered outliers.

We'll further check outliers by viewing the histograms from *Figure 13*. Jumping out is the distribution of AmountOfTotalClaim has two distinct peaks. That is, it is "bimodal". The peaks in any distribution are the most common number(s). The distribution of Total Claim Reported is due to multiple values occurring most frequently. Data values that occur the most often in a data set is the mode.

The second histogram from *Figure 13* superimposes the two events. The superimposed histogram follows the same bi-modal distribution as the single histogram. The outliers are of no concern and will not be removed from the data set.



```{python}


fig = plt.figure(figsize=(11, 6))
fig.suptitle('Insured Age Review', fontsize=11)
#sns.set(xlabel=None)
#sns.set(font_scale=0.5)
sns.set_style("dark")


plt.subplot(131)
plt.title('Box Plot-Insured Age and Reported Fraud', fontsize=7)
ia_1=sns.boxplot(data = fraud_v8, x = "InsuredAge", y='ReportedFraud')
ia_1.tick_params(axis='x', which='major', labelsize=5)
ia_1.tick_params(axis='y', labelsize=5)
ia_1.tick_params(axis='x', labelrotation=60)
ia_1.set(xlabel=None) 



plt.subplot(132)
plt.title('Histogram- Insured Age', fontsize=7)
ia_2=sns.histplot(data=fraud_v8, x="InsuredAge")
ia_2.tick_params(axis='x', which='major', labelsize=5)
ia_2.tick_params(labelrotation=60)
ia_2.tick_params(axis='y', labelsize=5)
ia_2.set(xlabel=None) 



plt.subplot(133)
plt.title('Histogram-Insured Age and Reported Fraud', fontsize=7)
ia_3=sns.histplot(data=fraud_v8, x="InsuredAge",hue="ReportedFraud")
ia_3.tick_params(axis='x', which='major', labelsize=5)
ia_3.tick_params(axis='y', labelsize=5)
#ac_3.tick_params(labelrotation=60)
ia_3.set(xlabel=None) 



plt.subplots_adjust(wspace=0.45)



```



```{python}







```


The box plots from *Figure 4* show outliers above the age of 60 for both reported fraud events. In addition, both histograms from *Figure 14* have slight skews to the right. Looking closer at the subplots, this appears to be due to drivers over the age of 50. Drivers over the age of 50 or 60 seeking auto insurance coverage is not unusual. Since the outliers are not unusual, they will not be removed or transformed.



```{python}


fig = plt.figure(figsize=(11, 6))
fig.suptitle('Policy Annual Premium  Review', fontsize=12)
#sns.set(xlabel=None)
#sns.set(font_scale=0.5)
sns.set_style("dark")


plt.subplot(131)
plt.title('Box Plot-Policy Annual Premium and Reported Fraud', fontsize=7)
pa_1=sns.boxplot(data = fraud_v8, x = "PolicyAnnualPremium", y='ReportedFraud')
pa_1.tick_params(axis='x', which='major', labelsize=5)
pa_1.tick_params(axis='y', labelsize=5)
pa_1.tick_params(axis='x', labelrotation=60)
pa_1.set(xlabel=None) 



plt.subplot(132)
plt.title('Histogram-Policy Annual Premium', fontsize=7)
pa_2=sns.histplot(data=fraud_v8,x="PolicyAnnualPremium")
pa_2.tick_params(axis='x', which='major', labelsize=5)
pa_2.tick_params(labelrotation=60)
pa_2.tick_params(axis='y', labelsize=5)
pa_2.set(xlabel=None) 



plt.subplot(133)
plt.title('Histogram-Policy Annual Premium and Reported Fraud', fontsize=7)
pa_3=sns.histplot(data=fraud_v8,x="PolicyAnnualPremium",hue="ReportedFraud")
pa_3.tick_params(axis='x', which='major', labelsize=5)
pa_3.tick_params(axis='y', labelsize=5)
#ac_3.tick_params(labelrotation=60)
pa_3.set(xlabel=None) 



plt.subplots_adjust(wspace=0.45)




```




```{python}









```


The boxplots from *Figure 15* both have outliers at the lower and higher ends. There are outliers on both higher and lower ends of both box plots. It's difficult to determine from the first histogram if there is a skew (tail). The mean of 1261 is slightly less than the median of 1266 which tells us there's a small skew to the left. There are a few small values of Policy Annual Premium that are driving the mean down. The third plot is of two histograms superimposed based on Reported Fraud event. Reported Fraud=Y skews slight to the left. The mean of 1255 is less than the median of 1271 which supports the left skew. The histogram for Reported Fraud=N appears normally distributed which is when the mean and median are the same. The mean and median for Reported Fraud=N are the same at 1263 thus normal distribution is confirmed.

Based on the statistical analysis, the skew of the first histogram is primarily caused by lower premiums of data points reported as fraud. This data can be important during a model building. We'll at addtional data for determining whether to keep these outliers.




```{python}


sns.set(style="darkgrid")

plt.figure(figsize=(10, 7))

# top bar -> sum all values(ReportedFraud=No and      # ReportedFraud=Yes) to find y position of the bars
total = fraud_v8.groupby('SeverityOfIncident')['count'].sum().reset_index()



# bar chart 1 -> top bars (group of #'ReportedFraud=No')
bar1 = sns.barplot(x="SeverityOfIncident",  y="count", data=total, color='darkblue')

# bottom bar ->  take only ReportedFraud=Yes values #from the data
fraud = fraud_v8[fraud_v8.ReportedFraud=='Y']

# bar chart 2 -> bottom bars (group of #'ReportedFraud=Yes')
bar2 = sns.barplot(x="SeverityOfIncident", y="count", data=fraud, estimator=sum, errorbar=None,  color='lightblue')

# add legend
top_bar = mpatches.Patch(color='darkblue', label='Fraud = No')
bottom_bar = mpatches.Patch(color='lightblue', label='Fraud = Yes')
plt.legend(handles=[top_bar, bottom_bar],fontsize=9, loc="upper right")



plt.tick_params(axis='x', which='major', labelsize=8, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=8)



plt.title(" Reported Fraud and Severity Of Incident", fontsize=15)
plt.xlabel(None)
plt.ylabel(None)



```



```{python}







```


<br/><br/>

*Figure 18* displays bar plots of categories belonging to the feature 'severity of incident' stacked based on whether fraud is 'Y' or 'N'. 'Major Damage' stands out as 60% of claims are reported as fraud whereas the other categories have claims reported as fraud under 16%.

<br/><br/>

```{python}


grouped_veh_mk=fraud_v8.groupby(['VehicleMake','ReportedFraud']).agg({'count':'sum'})



```




```{python}


grouped_veh_mk_perc=grouped_veh_mk.groupby(level=0, group_keys=False).apply(lambda x: x /x.sum()).round(2)



```




```{python}


grouped_veh_mk_perc.rename(columns={'count':'Percent'}, inplace=True)




```


Convert from multi index to single index.

```{python}


grouped_veh_mk_perc_single=grouped_veh_mk_perc.reset_index(level=[1])





```



```{python}



grouped_veh_mk_perc_single=grouped_veh_mk_perc_single.reset_index()



```

Pivot wider. This makes "Y" and "N" seperate columns


```{python}


grouped_veh_mk_perc_wide=grouped_veh_mk_perc_single.pivot(index='VehicleMake',columns='ReportedFraud',values='Percent').reset_index()



```


Reorder df following 'N'

```{python}



grouped_veh_mk_ordered=grouped_veh_mk_perc_wide.sort_values(by='N')



```




```{python}



my_range=range(1,len(grouped_veh_mk_ordered.index)+1)



```




```{python}



plt.figure(figsize=(9, 9))

plt.hlines(y=my_range, xmin=grouped_veh_mk_ordered['N'], xmax=grouped_veh_mk_ordered['Y'], color='grey', alpha=0.4)
plt.scatter(grouped_veh_mk_ordered['N'], my_range, color='skyblue', alpha=1, label='N')
plt.scatter(grouped_veh_mk_ordered['Y'], my_range, color='green', alpha=0.4 , label='Y')
plt.legend(title="Reported Fraud", loc="lower right", title_fontsize=18,fontsize=6, borderpad=0, facecolor="wheat")

plt.yticks(my_range, grouped_veh_mk_ordered['VehicleMake'])
plt.title("Reported Fraud by Vehicle Make", fontsize=15,loc='center')
plt.xlabel('Percent', fontsize=6)
plt.ylabel('None')


plt.tick_params(axis='x', which='major', labelsize=5, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=5)



```




```{python}







```




```{python}



plt.figure(figsize=(10,7))

sns.boxplot(x="VehicleMake", y="AmountOfTotalClaim", data=fraud_v8)

plt.tick_params(axis='x', which='major', labelsize=8, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=8)



plt.title("Total Claim by Vehicle Make", fontsize=14)
plt.xlabel(None)
plt.ylabel("Total Claim", fontsize=8)




```





```{python}








```



```{python}

plt.figure(figsize=(10,7))

sns.boxplot(x="VehicleMake", y="CapitalGains", data=fraud_v8)



plt.tick_params(axis='x', which='major', labelsize=8, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=8)



plt.title("Capital Gains by Vehicle Make", fontsize=16)
plt.xlabel(None)
plt.ylabel(None)





```





```{python}








```


<br/><br/>

*Figure 19* presents each vehicle make with percentages reported as fraud "Y" and "N". We find that Volkswagen, Mercedes, Ford, BMW, and Audi are the vehicle makes with reported fraud over 30%. This is an interesting statistic though due to the large number of categories we'll explore the 'Vehicle Make' feature further.

<br/><br/>

Box plots from *Figure 20* show the median total claims is roughly the same for all models.

<br/><br/>

We find from *Figure 21* that Nissan, Subaru, and Toyota have a median capital gain near 20,000, substantially larger than all other makes. The vehicle makes over 30% reported fraud from *Figure 19* all have zero medians.

Due to the number of categories of "Vehicle Make" we will exclude it from the modeling process.

<br/><br/>


```{python}



grouped_inc_st=fraud_v8.groupby(['IncidentState','ReportedFraud']).agg({'count':'sum'})




```





```{python}



grouped_inc_st_perc=grouped_inc_st.groupby(level=0, group_keys=False).apply(lambda x: x /x.sum()).round(2)




```





```{python}



grouped_inc_st_perc.rename(columns={'count':'Percent'}, inplace=True)




```




```{python}



grouped_inc_st_perc_single=grouped_inc_st_perc.reset_index(level=[1])





```




```{python}



grouped_inc_st_perc_single=grouped_inc_st_perc_single.reset_index()



```




```{python}


grouped_inc_st_perc_wide=grouped_inc_st_perc_single.pivot(index='IncidentState',columns='ReportedFraud',values='Percent').reset_index()



```




```{python}


grouped_inc_st_ordered=grouped_inc_st_perc_wide.sort_values(by='N')




```



```{python}


my_range_2=range(1,len(grouped_inc_st_ordered.index)+1)




```




```{python}



plt.figure(figsize=(9, 9))

plt.hlines(y=my_range_2, xmin=grouped_inc_st_ordered['N'], xmax=grouped_inc_st_ordered['Y'], color='grey', alpha=0.4)
plt.scatter(grouped_inc_st_ordered['N'], my_range_2, color='skyblue', alpha=1, label='N')
plt.scatter(grouped_inc_st_ordered['Y'], my_range_2, color='green', alpha=0.4 , label='Y')
plt.legend(title="Reported Fraud", loc="lower right", title_fontsize=8,fontsize=6, borderpad=0, facecolor="wheat")

plt.yticks(my_range_2, grouped_inc_st_ordered['IncidentState'])
plt.title("Reported Fraud by Incident State", loc='center')
plt.xlabel('Percent', fontsize=6)
plt.ylabel('None')


plt.tick_params(axis='x', which='major', labelsize=5, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=5)



```



```{python}







```



```{python}




plt.figure(figsize=(11,11), dpi=80)

sns.boxplot(x="IncidentState", y="AmountOfTotalClaim", data=fraud_v8)

plt.tick_params(axis='x', which='major', labelsize=7, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=7)



plt.title("Total Claim by Incident State", fontsize=15)
plt.xlabel(None)
plt.ylabel("Total Claim", fontsize=8)



```




```{python}







```





```{python}


plt.figure(figsize=(12,9), dpi=80)

sns.boxplot(x="IncidentState", y="CapitalGains", data=fraud_v8)



plt.tick_params(axis='x', which='major', labelsize=8, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=8)



plt.title("Capital Gains by Incident State", fontsize=14)
plt.xlabel(None)
plt.ylabel(None)




```




```{python}








```


<br/><br/>

From *Figure 22* we find there are four incident states in which claims reported as fraud is over 30%. State3 has over 40% claims reported as fraud.

*Figure 23* and *Figure 24* do not exhibit differences of vehicle makes in regards to total claims and capital gains.

We'll retain the 'Incident State' feature as it has half the categories as 'Vehicle Make".

<br/><br/>



```{python}




grouped_type_inc=fraud_v8.groupby(['TypeOfIncident','ReportedFraud']).agg({'count':'sum'})



```





```{python}


grouped_type_inc_perc=grouped_type_inc.groupby(level=0, group_keys=False).apply(lambda x: x /x.sum()).round(2)






```





```{python}




grouped_type_inc_perc.rename(columns={'count':'Percent'}, inplace=True)




```




```{python}



grouped_type_inc_perc_single=grouped_type_inc_perc.reset_index(level=[1])




```




```{python}



grouped_type_inc_perc_single=grouped_type_inc_perc_single.reset_index()




```






```{python}



grouped_type_inc_perc_wide=grouped_type_inc_perc_single.pivot(index='TypeOfIncident',columns='ReportedFraud',values='Percent').reset_index()




```





```{python}




grouped_type_inc_ordered=grouped_type_inc_perc_wide.sort_values(by='N')



```




```{python}



my_range_3=range(1,len(grouped_type_inc_ordered.index)+1)



```




```{python}


plt.figure(figsize=(9, 9))

plt.hlines(y=my_range_3, xmin=grouped_type_inc_ordered['N'], xmax=grouped_type_inc_ordered['Y'], color='grey', alpha=0.4)
plt.scatter(grouped_type_inc_ordered['N'], my_range_3, color='skyblue', alpha=1, label='N')
plt.scatter(grouped_type_inc_ordered['Y'], my_range_3, color='green', alpha=0.4 , label='Y')
plt.legend(title="Reported Fraud", loc="lower right", title_fontsize=8,fontsize=6, borderpad=0, facecolor="wheat")

plt.yticks(my_range_3, grouped_type_inc_ordered['TypeOfIncident'])
plt.title("Reported Fraud by Type of Incident", loc='center')
plt.xlabel('Percent', fontsize=6)
plt.ylabel('None')

plt.tick_params(axis='x', which='major', labelsize=5, labelrotation=90)

plt.tick_params(axis='y', which='major', labelsize=5)



```




```{python}







```



<br/><br/>

From *Figure 25* two categories stand out with respect to reported fraud. 'Single Vehicle Collision' and 'Multi-vehicle collision' from the feature 'Type of Incident' have claims reported as fraud at 31% and 29% respectively. The other two categories are under 14%.

<br/><br/>

# Data Preprocessing

<br/><br/> 

Before model building can start, we'll need to perform pre-processing. This will entail splitting our data into training, validation, and test sets along with transforming numerical and categorical features into classification friendly formats.

<br/><br/>



```{python}





from sklearn.model_selection import train_test_split


from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder


from sklearn.compose import ColumnTransformer, make_column_transformer

from sklearn.pipeline import Pipeline, make_pipeline

from sklearn.model_selection import StratifiedKFold

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

from sklearn.compose import make_column_selector as selector

from sklearn.preprocessing import OneHotEncoder




```




```{python}



model_data=fraud_v8.copy()



```



```{python}


from sklearn import set_config


```



```{python}



model_data=model_data.drop(['IncidentCity','AmountOfInjuryClaim', 'AmountOfPropertyClaim', 'UmbrellaLimit','AmountOfVehicleDamage', 'VehicleModel',
'VehicleYOM', 'count', 'InsuredEducationLevel','InsuredOccupation', 'VehicleMake'], axis=1)


```





```{python}


print('The  Target categories: {}:'.format(model_data['ReportedFraud'].cat.categories))



```

## Splt Dataset


<br/><br/>

We will separate the data to get predictor features and target features into separate data frames.

<br/><br/>



```{python}


X=model_data.drop("ReportedFraud", axis=1)

y=model_data["ReportedFraud"]



```


The data type of the target feature is categorical. Most machine learning algorithms require numerical data types. Label Encoder will be used to transform y to a numeric tpye.

<br/><br/>


```{python}

label_encoder=LabelEncoder()





```




```{python}




Transform y to data frame from series

<br/><br/>


```



```{python}


y=pd.DataFrame(y)



```



```{python}


y["ReportedFraud"]=label_encoder.fit_transform(y["ReportedFraud"])


```




```{python}


y["ReportedFraud"].dtype



```





```{python}


print('Target Feature categories (N,Y) as binary:  {}:'.format(y["ReportedFraud"].cat.categories))



```

From the two above outputs we can see that the target feature has been converted into binary form though its data type is integer. The data type must be converted back to categorical.


```{python}


y["ReportedFraud"]=y["ReportedFraud"].astype("category")



```



```{python}


print('Target Feature categories as binary:  {}:'.format(y["ReportedFraud"].cat.categories))


```



```{python}


print('Target Feature as categorical binary:\n \n')

y.info()



```




```{python}


print('Shape of Predictor Features is {}:'.format(X.shape))



```




```{python}



print('Shape of Target Feature is {}:'.format(y.shape))


```




<br/><br/>

The makup of the X data frame is 28836 rows and 26 columns. The y data frame has the same number of rows, 28836, and one column, the target feature.

<br/><br/>




```{python}



print("*********** X Structure***********")

X.info()


```

We will now split X,y into Train, Validation and Test sets

<br/><br/>

```{python}




X_train, X_rem, y_train, y_rem=train_test_split(X,y, train_size=0.70)


```



```{python}



X_valid, X_test, y_valid, y_test=train_test_split(X_rem, y_rem, test_size=0.5)



```





```{python}



print('Shape of X Train {}:'.format(X_train.shape))

print('Shape of X Valid {}:'.format(X_valid.shape))

print('Shape of X Test {}:'.format(X_test.shape))

print('Shape of y Train {}:'.format(y_train.shape))

print('Shape of y Valid {}:'.format(y_valid.shape))

print('Shape of y Test {}:'.format(y_test.shape))




```



```{python}



y_train_np=np.array(y_train)

y_valid_np=np.array(y_valid)

y_test_np=np.array(y_test)




```


<br/><br/>

y features will be transformed to numpy array

<br/><br/>

```{python}





print('Shape of y Train rv {}:'.format(y_train_np.shape))
print('Shape of y Valid rv {}:'.format(y_valid_np.shape))

print('Shape of y Test rv {}:'.format(y_test_np.shape))


```





```{python}


<br/><br/>

Next we will transform y features to one dimensional arrays

<br/><br/>




```





```{python}



y_train_rv=np.ravel(y_train_np)

y_valid_rv=np.ravel(y_valid_np)

y_test_rv=np.ravel(y_test_np)



```





```{python}


print('Shape of y Train rv {}:'.format(y_train_rv.shape))
print('Shape of y Valid rv {}:'.format(y_valid_rv.shape))

print('Shape of y Test rv {}:'.format(y_test_rv.shape))



```


<br/><br/>

From the above output we see that y train, y valid, and y test have been transformed into one dimensional numpy arrays.

<br/><br/>

<br/><br/>

## Transform Categorical and Numerical features

<br/><br/>

Our next step is to transform the predictor features into acceptable machine learning formats.

Transformation for numerical features is performed by scaling. Scaling prevents a feature with a range let's say in the thousands from being considered more important than a feature having a lower range. Scaling places features at the same importance before being applied to a machine learning algorithm. There are different methods used in scaling features, for this analysis we'll be using standard scaling. Standard scaling transforms the data to have zero mean and a variance of one, thus making the data unitless.

Most machine learning algorithms only accept numerical features which makes categorical features unacceptable in their original form. Thus, we need to encode categorical features into numerical values. The act of replacing categories with numbers is called categorical encoding. For this we will use one-hot encoding. Categorical features are represented as a group of binary features, where each binary feature represents one category. The binary feature takes the integer value 1 if the category is present, or 0 otherwise.

<br/><br/>

<!--#  set_config  configures pre-proccessing steps such as Standard Scaler and One Hot Encoding  to return a Pandas DataFrame   -->

set_config configures pre-proccesing steps such as Standard Scaler and One Hot Encoding to return a Pandas DataFrame.

<br/><br/>




```{python}



set_config(transform_output="pandas")




```



```{python}



#Define classification columns
categorical = list(X_train.select_dtypes('category').columns)
print(f"Categorical columns are: {categorical}")



```





```{python}


#Define numeric columns
numerical = list(X_train.select_dtypes('number').columns)
print(f"Numerical columns are: {numerical}")






```



<br/><br/>

First, we will create transformed train, valid, and test sets for the Logistic Regression model. This entails dropping the first category of each feature during One Hot Encoding.


```{python}



ct_lr=ColumnTransformer(
  transformers=[
   ('scale',StandardScaler(), numerical),
   ('ohe',OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'), categorical)
]
)



```







```{python}

X_train_lr=ct_lr.fit_transform(X_train)






```





```{python}



print('************First Five Rows X_train_lr************')
print(X_train_lr.head())



```





```{python}



X_valid_lr=ct_lr.transform(X_valid)



```




```{python}



print('************First Five Rows X_valid_lr************')
print(X_valid_lr.head())




```





```{python}




X_test_lr=ct_lr.transform(X_test)


```




```{python}



print('************First Five Rows X_test_lr************')


print(X_test_lr.head())



```

<br/><br/>

<br/><br/>

We see from the first five rows of the train, valid, and test sets that the features have been transformed while at the same time retaining the column feature names.


```{python}



print('Shape of X Train lr {}:'.format(X_train_lr.shape))

print('Shape of X Valid lr {}:'.format(X_valid_lr.shape))

print('Shape of X Test lr {}:'.format(X_test_lr.shape))



```




```{python}




Next, we transform training, valid, and test sets for all other models. During OneHotEncoding, the first category will be dropped only if the feature is binary.

<br/><br/>




```




```{python}


ct_tr=ColumnTransformer(
  transformers=[
   ('num',StandardScaler(), numerical),
   ('cat',OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='if_binary'), categorical)
]
)




```


Transform X_train

<br/><br/>

```{python}


X_train_tr=ct_tr.fit_transform(X_train)





```




```{python}



print('************First Five Rows X_train_tr************')


print(X_train_tr.head())




```





```{python}



X_valid_tr=ct_tr.transform(X_valid)




```





```{python}





print('************First Five Rows X_valid_tr************')

print(X_valid_tr.head())



```





```{python}



X_test_tr=ct_tr.transform(X_test)




```






```{python}





print('************First Five Rows X_test_tr************')

print(X_test_tr.head())



```




```{python}




print('Shape of X Train tr {}:'.format(X_train_tr.shape))

print('Shape of X Valid tr {}:'.format(X_valid_tr.shape))

print('Shape of X Test tr {}:'.format(X_test_tr.shape))




```


<br/><br/>

From the shape output we find there are 13 additional columns compared to the logistic regression transformed data.

<br/><br/>



```{python}



from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, roc_auc_score, recall_score, RocCurveDisplay, precision_score, f1_score




```





```{python}


from sklearn import metrics

from sklearn.inspection import permutation_importance








```




# Models

br/><br/>

For the purpose of evaluating model performance, the event of interest we are interested in is if reported fraud is yes. This is considered the positive class. Classification metrics are used to determine how well our models predict the event of interest.

## Metrics Definitions


Accuracy-measures the number of predictions that are correct as a percentage of the total number of predictions that are made. As an example, if 90% of your predictions are correct, your accuracy is simply 90%. Calculation: number of correct predictions/Number of total predictions. TP+TN/(TP+TN+FP+FN)

Precision-tells us about the quality of positive predictions. It may not find all the positives but the ones that the model does classify as positive are very likely to be correct. As an example, out of everyone predicted to have defaulted, how many of them actually did default? So within everything that has been predicted as a positive, precision counts the percentage that is correct. Calculation: True positives/All Positives. TP/(TP+FP)



Recall- tells us about how well the model identifies true positives. The model may find a lot of positives yet it also will wrongly detects many positives that are not actually positives. Out of all the patients who have the disease, how many were correctly identified? So within everything that actually is positive, how many did the model successfully to find. A model with low recall is not able to find all (or a large part) of the positive cases in the data. Calculated as: True Positives/(False Negatives + True Positives)



F1 Score-The F1 score is defined as the harmonic mean of precision and recall.

The harmonic mean is an alternative metric for the more common arithmetic mean. It is often useful when computing an average rate. <https://en.wikipedia.org/wiki/Harmonic_mean>

The formula for the F1 score is the following: 2 times((precision\*Recall)/(Precision + Recall))

Since the F1 score is an average of Precision and Recall, it means that the F1 score gives equal weight to Precision and Recall:

<br/><br/> 

<br/><br/>



# Model Training and Validation




```{python}



from sklearn.linear_model import LogisticRegression


```





```{python}

logreg=LogisticRegression()



```




```{python}



lr_params={
'C': [0.001, 0.01, 0.1, 1, 10], 
'penalty': ['l2'],
'max_iter': list(range(10000,15000, 20000)),
'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']

}


```



```{python}




lr_search=RandomizedSearchCV(logreg, lr_params, refit=True, 
verbose=3,cv=5,n_iter=2, scoring='roc_auc',return_train_score=True, n_jobs=-1)



```




```{python}



lr_search.fit(X_train_lr, y_train_rv)



```




```{python}



lr_clf=lr_search.best_estimator_




```




```{python}


lr_clf




```


<br/><br/>

The above displays gives us the parameters chosen for the logistic regression  model. 

<br/><br/>



```{python}




y_pred_valid_lr=lr_clf.predict(X_valid_lr)


```



###  Validation Metrics


```{python}




print('****Logistic RegressionValidation Classification Report****')


print(classification_report(y_valid_rv, y_pred_valid_lr
))




```




```{python}





log_precision=precision_score(y_valid_rv, y_pred_valid_lr).round(2)


```




```{python}




log_recall=recall_score(y_valid_rv, y_pred_valid_lr).round(2)




```






```{python}



log_f1=f1_score(y_valid_rv, y_pred_valid_lr).round(2)





```







```{python}



log_accuracy=accuracy_score(y_valid_rv, y_pred_valid_lr).round(2)




```




```{python}



cm_lr_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_lr, labels=[0,1])
df_cm_lr_vl = pd.DataFrame(cm_lr_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_lr_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_lr_vl.flatten()/np.sum(cm_lr_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,6))
sns.heatmap(df_cm_lr_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')

plt.title("Confusion Matrix-Logistic Regression", fontsize=14)



```





```{python}







```


### Feature Importance

We will now look at Feature Importance. Feature Importance is a score assigned to the features of a Machine Learning model that defines how "important" is a feature to the model's prediction.

```{python}



feature_importance_lr=pd.DataFrame({'feature':list(X_valid_lr.columns),'feature_importance':[abs(i) for i in lr_clf.coef_[0]]})


feature_importance_lr=feature_importance_lr.sort_values('feature_importance',ascending=False)




```



For the logistcal regression model we took the absolute value of the coefficients so as to get the Importance of the feature both with negative and positive effect.

<br/><br/>

Now that we have the importance of the features we will now transform the coefficients for easier interpretation. The coefficients are in log odds format. We will transform them to odds-ratio format.

<br/><br/>

Combine feature names and coefficients on top Pandas DataFrame



```{python}




feature_names_lr=pd.DataFrame(X_valid_lr.columns, columns=['Feature'])

log_coef=pd.DataFrame(np.transpose(lr_clf.coef_), columns=['Coefficent'])

coefficients=pd.concat([feature_names_lr, log_coef], axis=1)



```


<br/><br/> Calculate exponent of the logistic regression coefficients


```{python}



coefficients['Exp_Coefficient']=np.exp(coefficients['Coefficent'])





```


<br/><br/>

Remove coefficients that are equal to zero.

<br/><br/>

```{python}




coefficients=coefficients[coefficients['Exp_Coefficient']>=1]


```



```{python}



print('******************Top Five Coefficients******************')

print(coefficients_tp5[['Feature', 'Exp_Coefficient']])




```


## Support Vector Machine



<br/><br/>

Support Vector Machine (the "road machine") is responsible for finding
the decision boundary to separate different classes and maximize the
margin. A decision boundary differentiates two classes. A data point falling on either side of the decision boundary can be attributed to different classes.  Binary classes would be either yes or no. 

<br/><br/>

```{python}



from sklearn.svm import SVC





```




```{python}


param_grid_svc = {'C': [.01,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}





```





```{python}



svc_ln=SVC(random_state=0, probability=True)



```




```{python}



svc_ln=SVC(random_state=0)




```




```{python}



grid_svc=RandomizedSearchCV(svc_ln,param_grid_svc, refit=True, 
verbose=3,cv=5,n_iter=2, scoring='roc_auc',return_train_score=True, n_jobs=-1)





```





```{python}



grid_svc.fit(X_train_tr, y_train_rv)




```




```{python}


svc_clf=grid_svc.best_estimator_




```





```{python}



svc_clf





```

<br/><br/>

The above displays gives us the parameters chosen for the support vector machine Model. 

<br/><br/>


### Validation Metrics

```{python}


y_pred_valid_svc=svc_clf.predict(X_valid_tr)





```







```{python}




print('**********SVC Validation Classification Report**********')

print(classification_report(y_valid_rv, y_pred_valid_svc))



```







```{python}


svc_recall=recall_score(y_valid_rv, y_pred_valid_svc).round(2)





```







```{python}




svc_precision=precision_score(y_valid_rv, y_pred_valid_svc).round(2)




```





```{python}



svc_f1=f1_score(y_valid_rv, y_pred_valid_svc).round(2)




```





```{python}




svc_accuracy=accuracy_score(y_valid_rv, y_pred_valid_svc).round(2)





```



## Decision Tree




Decision trees use a flowchart like a tree structure to show the
predictions that result from a series of feature splits. To
accomplish this, a decision tree is made up of three types of nodes:

-   Root Node (parent node): The node that starts the graph. It
    evaluates the variable that best splits the data.

-   Intermediate Nodes (child nodes): These are nodes where features are
    evaluated for further splits of the data but are not the final
    nodes.

-   Leaf nodes (terminal nodes): These are the final nodes of the tree,
    where the prediction of a categorical event are made.

For a more detailed explanation of decision trees check the link below.

[Guide to Decision
Trees](https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/)



```{python}




from sklearn.tree import DecisionTreeClassifier, plot_tree

from sklearn import tree



```



### Training

<br/><br/>

Parameter Definitions:

min_samples_split - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.

min_samples_leaf - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches.Defines the minimum samples (or observations) required in a terminal node or leaf.

max_depth - Determines the length of the tree which is the same as the number of splitting rounds. T

max_features - The number of features to consider when looking for the best split



```{python}




tree_params={
  'max_features':[0.25, 0.50,0.75,1.0],
  'min_samples_split':[2,4,6,10,12],
  'max_depth':[250, 500, 750,1000],
  'min_samples_leaf':[3,4,5,7]}




```







```{python}




dt=DecisionTreeClassifier()




```





```{python}




dt_grid=GridSearchCV(estimator=dt,param_grid=tree_params, scoring='roc_auc', cv=5, verbose=3, refit=True, return_train_score=True, n_jobs=-1)



```






```{python}


dt_grid.fit(X_train_tr, y_train_rv)






```







```{python}




dt_clf=dt_grid.best_estimator_




```





```{python}



dt_clf



```

<br/><br/>

The above displays gives us the parameters chosen for the Decision Tree Model. 


<br/><br/>





```{python}




y_pred_valid_dt = dt_clf.predict(X_valid_tr)



```




### Validation Metrics


```{python}



print('****Decision Tree Classification Report****')


print(classification_report(y_valid_rv, y_pred_valid_dt))




```




```{python}



dt_recall=recall_score(y_valid_rv, y_pred_valid_dt).round(2)





```






```{python}




dt_precision=precision_score(y_valid_rv, y_pred_valid_dt).round(2)






```


```{python}


dt_f1=f1_score(y_valid_rv, y_pred_valid_dt).round(2)




```



```{python}



dt_accuracy=accuracy_score(y_valid_rv, y_pred_valid_dt).round(2)




```




```{python}


tree_auc_valid=roc_auc_score(y_valid_rv,dt_clf.predict_proba(X_valid_tr)[:,1]).round(2)



```




```{python}



print("Decision Tree Area Under the Curve: (%.2f)" % tree_auc_valid)



```



```{python}



cm_dt_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_dt, labels=[0,1])
df_cm_dt_vl = pd.DataFrame(cm_dt_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_dt_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_dt_vl.flatten()/np.sum(cm_dt_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_dt_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')

plt.title("Confusion Matrix-Decision Tree", fontsize=14)



```



```{python}







```




### eature Importance

```{python}



# let's create a dictionary of features and their importance values
feat_dict= {}
for col, val in sorted(zip(X_train_tr.columns, dt_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict[col]=val



```




```{python}



feat_dt_df = pd.DataFrame({'Feature':feat_dict.keys(),'Importance':feat_dict.values()})



```




```{python}




feat_dt_tp5=feat_dt_df.nlargest(5,"Importance")




```




```{python}



values = feat_dt_tp5.Importance    
idx = feat_dt_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features Decision Tree Model')

plt.ylabel("Features", fontsize=9)

plt.tick_params(axis='x', which='major', labelsize=8)

plt.tick_params(axis='y', which='major', labelsize=7,labelrotation=45)




```






```{python}









```


## Random Forest
l

<br/><br/>

Random forest is an ensemble learning method. Ensemble learning takes predictions from multiple models are merges them  to enhance the accuracy  of prediction.  There are four types of ensemble techniques.  We'll be using Bagging (which random forest is an example of) and boosting,  which our next four models will be an example of.  


Bagging involves fitting many decision trees on different samples of the same dataset and averaging the predictions.




Random Forest models are made up of individual decision trees whose
predictions are combined for a final result. The final result is decided
using majority rules which means that the final prediction is what the
majority of the decision tree models chose. Random Forests can be made
up of thousands of decision trees.

Simply put, the random forest builds multiple decision trees and merges
them together to get a more accurate prediction.

[Random Forest for
Beginners](https://www.analyticsvidhya.com/blog/2021/10/an-introduction-to-random-forest-algorithm-for-beginners/)

<br/><br/>


```{python}




from sklearn.ensemble import RandomForestClassifier




```


<br/><br/>

The individual models of random forest are decision trees. The decision trees predictions are combined for reaching a result. Majority rules is the process by which the Random Forest Classifier determines the result. An example would be 5 models in which 3 of the 5 models predict 'yes' for the classification problem.

<br/><br/>

### Training


```{python}



<br/><br/>



Parameter Definitions (Not previously defined):

n_estimators - the number of trees to construct

<br/><br/>



```





```{python}



rf_params={'n_estimators':[500,1000,22500,5000],          
'max_features':[0.25,0.50,0.75,1.0],
'min_samples_split':[2,4,6,8], 
'max_depth': [500, 1000, 2000, 4000], 
'min_samples_leaf': [3, 4, 5, 6] }




```




```{python}



rf=RandomForestClassifier(n_jobs=-1)




```






```{python}




rf_search=RandomizedSearchCV(rf, rf_params, n_iter=2,refit=True, 
verbose=3,cv=5, scoring='roc_auc',return_train_score=True, n_jobs=-1)



```






```{python}



rf_search.fit(X_train_tr, y_train_rv)






```






```{python}



rf_clf=rf_search.best_estimator_



```





```{python}




rf_clf




```




<br/><br/>

The above display presents the parameters chosen for the random Forest classifier. 



<br/><br/>

```{python}


rf_search.best_params_




```




```{python}


y_pred_valid_rf=rf_clf.predict(X_valid_tr)




```




### Validation Metrics



```{python}



print('****Random Forest Validation Classification Report****')

print(classification_report(y_valid_rv, y_pred_valid_rf))



```






```{python}



rf_recall=recall_score(y_valid_rv, y_pred_valid_rf).round(2)




```






```{python}




rf_precision=precision_score(y_valid_rv, y_pred_valid_rf).round(2)



```





```{python}



rf_f1=f1_score(y_valid_rv, y_pred_valid_rf).round(2)



```





```{python}



rf_accuracy=accuracy_score(y_valid_rv, y_pred_valid_rf).round(2)




```






```{python}




valid_rf_auc_score=roc_auc_score(y_valid_rv,rf_clf.predict_proba(X_valid_tr)[:,1]).round(2)




```




```{python}



valid_rf_auc_score




```





```{python}



cm_rf_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_rf, labels=[0,1])
df_cm_rf_vl = pd.DataFrame(cm_rf_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_rf_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_rf_vl.flatten()/np.sum(cm_rf_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_rf_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')

plt.title("Confusion Matrix-Random Forest", fontsize=14)






```







```{python}










```



### Feature Importance


```{python}




# let's create a dictionary of features and their importance values
feat_dict_rf= {}
for col, val in sorted(zip(X_train_tr.columns, rf_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict_rf[col]=val





```






```{python}



feat_rf_df = pd.DataFrame({'Feature':feat_dict_rf.keys(),'Importance':feat_dict_rf.values()})




```




```{python}


feat_rf_tp5=feat_rf_df.nlargest(5,"Importance")




```





```{python}




values = feat_rf_tp5.Importance    
idx = feat_rf_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features Random Forest Model')

plt.ylabel("Features", fontsize=510)

plt.tick_params(axis='x', which='major', labelsize=9)

plt.tick_params(axis='y', labelsize=8,labelrotation=45)




```




```{python}







```



##  AdaBoost

<br/><br/>

Boosting learns from the mistakes of individual trees. Each new tree is built from the previous tree. We'll be using five boosting algorithms, the first being AdaBoost.

In AdaBooost, a new tree adjusts based on the previous tree by adjusting its weights based on errors from that previous tree. Observations have an assigned weight, and each tree is built in an additive manner, assigning greater weights (more importance) to misclassified observations in the previous learners. Misclassified would be predicted yes but actual is no. False Errors from previous trees are weak learners. Weak learners perform no better tha

```{python}



from sklearn.ensemble import AdaBoostClassifier




```





```{python}



adb=AdaBoostClassifier(random_state=1)



```




### Training

Parameter Definitions (Not previously defined)

Learning rate - Shrinks the contribution of individual trees for each round of boosting so that no tree has too much influence. Basically, learning rate limits the influence of individual trees.  By lowering the learning rate, more trees are required to produce better scores. Lowering learning rate prevents over fitting because the size of weights carried forward is smaller.

```{python}





base_tree = DecisionTreeClassifier(max_depth=6)






```




```{python}


adb=AdaBoostClassifier(random_state=1,estimator=base_tree)





```





```{python}




adb_params={
  'n_estimators':[1000, 3000, 5000, 8000, 10000],
  'learning_rate':(0.0001,.001,.01,0.3, 0.4, 0.5,1)
  
}


```




```{python}




adb_grid=RandomizedSearchCV(estimator=adb,
param_distributions=adb_params,n_iter=2, scoring='roc_auc', cv=5, verbose=3, refit=True, n_jobs=-1)



```





```{python}



adb_grid.fit(X_train_tr, y_train_rv)




```






```{python}



adb_clf=adb_grid.best_estimator_



```




```{python}



adb_clf




```


<br/><br/>


The above display presents the paramters chosen for the AdaBoost model. 

<br/><br/>




```{python}




adb_grid.best_params_



```

### Validation Metrics


```{python}



y_pred_valid_adb=adb_clf.predict(X_valid_tr)




```





```{python}




print('****AdaBoost Validation Classification Report****')
print(classification_report(y_valid_rv, y_pred_valid_adb))



```




```{python}




adb_recall=recall_score(y_valid_rv, y_pred_valid_adb).round(2)




```




```{python}



adb_precision=precision_score(y_valid_rv, y_pred_valid_adb).round(2)






```




```{python}



adb_f1=f1_score(y_valid_rv, y_pred_valid_adb).round(2)



```




```{python}


adb_accuracy=accuracy_score(y_valid_rv, y_pred_valid_adb).round(2)




```




```{python}


cm_adb_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_adb, labels=[0,1])
df_cm_adb_vl = pd.DataFrame(cm_adb_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_adb_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_adb_vl.flatten()/np.sum(cm_adb_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_adb_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')





```




```{python}









```


### Feature Importance


```{python}




feat_dict_adb= {}
for col, val in sorted(zip(X_train_tr.columns,adb_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict_adb[col]=val




```





```{python}




feat_adb_df = pd.DataFrame({'Feature':feat_dict_adb.keys(),'Importance':feat_dict_adb.values()})




```






```{python}



feat_adb_tp5=feat_adb_df.nlargest(5,"Importance")




```





```{python}




values = feat_adb_tp5.Importance    
idx = feat_adb_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features  AdaBoost Model')

plt.ylabel("Features", fontsize=9)

plt.tick_params(axis='x', which='major', labelsize=8)

plt.tick_params(axis='y', labelsize=8, labelrotation=45)





```




```{python}








```



## Gradient Boosting


<br/><br/>

Gradient boosting also uses incorrect predictions from previous trees to adjust the next tree though this is accomplished by fitting each new tree based on the errors of the previous tree's predictions. Mistakes from the previous trees are used to build a new tree solely around these mistakes. As mentioned early in AdaBoost, gradient boosting is taking these errors (weak learner) and making them a strong learner.  The difference is the gradient boost algorithm only uses the errors from the previous tree in contrast to AdaBoost. 

The main idea behind this algorithm is to build models sequentially and these subsequent models try to reduce the errors of the previous model. Errors are reduced  by building a new model on the errors or residuals of the previous model.



<br/><br/>


### Training

<br/><br/>

parameter definitions (not previously defined)

Criterion - The loss function used to find the optimal feature and threshold to split the data



base learner - Is the initial decision tree. It's the first leaner in the process
 Subsample - A subset of samples. A subset of rows means not all rows may be included when building each tree. The percentage of each boosting round is limited.
 
 max_leaf_nodes - The maximum number of terminal nodes or leaves in a tree.

n_iter_no_change - is used to decide if early stopping will be used to terminate training when validation score is not improving.

tol - Tolerance for the early stopping. When the loss is not improving by at least tol for n_iter_no_change iterations (if set to a number), the training stops.

ccp_alpha - Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen



```{python}


from sklearn.ensemble import GradientBoostingClassifier





```




```{python}



gb=GradientBoostingClassifier(warm_start=True)




```





```{python}




gb_params={
  'subsample':[0.4, 0.6, 0.7, 0.75],
  'n_estimators':np.arange(500, 5000, 500),
  'learning_rate':[0.01,0.05, 0.075,0.1],
  'max_features':range(7,20,2),
  'min_samples_split':range(1000,2100,200),
  'min_samples_leaf':range(30,71,10),
  'max_depth':range(5,16,2),
  
}



```




```{python}


search_gb=RandomizedSearchCV(estimator=gb,
param_distributions=gb_params,n_iter=2, scoring='roc_auc', cv=5, verbose=1, refit=True, n_jobs=-1, random_state=2)



```




```{python}


search_gb.fit(X_train_tr, y_train_rv)





```




```{python}


gb_clf=search_gb.best_estimator_



```



```{python}


gb_clf



```



<br/><br/>

The above display presents the parameters chosen for the gradient boost model. 

<br/><br/>




###  Validation Metrics



```{python}


print('****Gradient Boosting Classification Report****')

print(classification_report(y_valid_rv, y_pred_valid_gb))

```




```{python}


gb_recall=recall_score(y_valid_rv, y_pred_valid_gb).round(2)




```




```{python}



gb_precision=precision_score(y_valid_rv, y_pred_valid_gb).round(2)


```



```{python}


gb_f1=f1_score(y_valid_rv, y_pred_valid_gb).round(2)



```



```{python}



gb_accuracy=accuracy_score(y_valid_rv, y_pred_valid_gb).round(2)



```




```{python}


valid_gb_auc_score=roc_auc_score(y_valid_rv,gb_clf.predict_proba(X_valid_tr)[:,1]).round(2)



```




```{python}


valid_gb_auc_score



```




```{python}



cm_gb_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_gb, labels=[0,1])
df_cm_gb_vl = pd.DataFrame(cm_gb_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_gb_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_gb_vl.flatten()/np.sum(cm_gb_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_gb_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')


plt.title("Confusion Matrix-Gradient Boost", fontsize=14)



```



```{python}







```


### Feature Importance

```{python}



#  create a dictionary of features and their importance values
feat_dict_gb = {}
for col, val in sorted(zip(X_train_tr.columns,gb_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict_gb[col]=val



```





```{python}



feat_gb_df= pd.DataFrame({'Feature':feat_dict_gb.keys(),'Importance':feat_dict_gb.values()})



```




```{python}




feat_gb_tp5=feat_gb_df.nlargest(5,"Importance")



```




```{python}


values = feat_gb_tp5.Importance    
idx = feat_gb_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features  Gradient Boosting Model')

plt.ylabel("Features", fontsize=8)

plt.tick_params(axis='x', which='major', labelsize=8)

plt.tick_params(axis='y', labelsize=8, labelrotation=45)



```



```{python}







```




## Extreme Gradient Boosting

<br/><br/>

Extreme Gradient boosting is similar to gradient boosting with a few improvements. First, enhancements make it faster than other ensemble methods. Secondly, built-in regularization allows it to have an advantage in accuracy. Regularization is the process of adding information to reduce variance and prevent over fitting.

<br/><br/>

```{python}



from xgboost import XGBClassifier



```


### Training






<br/><br/>

Paramter Definitions (Not previously defined):



<br/><br/>


```{python}


params_xg={
    "learning_rate": [0.01, 0.05, 0.10, 0.20,0.25,0.30],
    "max_depth": [3,4,5,6,8,10,12,15],
    "min_child_weight": [1,3,5,7],
    "gamma": [0.0,0.01,0.05,0.1,0.5,1,2,3],
    "colsample_bytree": [0.5,0.6,0.7,0.8,0.9],
    "n_estimators":np.arange(500, 5000, 500),
    'subsample': [0.7,0.8,0.9,1],
  
  
}



```





```{python}



estimator_xg=XGBClassifier(objective='binary:logistic', n_jobs=-1)


```




```{python}


search_xg=RandomizedSearchCV(estimator=estimator_xg,
param_distributions=params_xg,n_iter=2, scoring='roc_auc', cv=5, verbose=3, refit=True, n_jobs=-1)


```




```{python}


search_xg.fit(X_train_tr, y_train_rv)



```



```{python}


xg_clf=search_xg.best_estimator_


```




```{python}



xg_clf


```

br/><br/>


The above display presents the parameters chosen for the XGBoost model. 

<br/><br/>

```{python}


y_pred_valid_xg=xg_clf.predict(X_valid_tr)



```

### Validation Metrics


```{python}



print('****XGBoost Validation Report Classification Report****')


print(classification_report(y_valid_rv, y_pred_valid_xg))


```




```{python}



xg_recall=recall_score(y_valid_rv, y_pred_valid_xg).round(2)


```




```{python}



xg_precision=precision_score(y_valid_rv, y_pred_valid_xg).round(2)


```




```{python}



xg_f1=f1_score(y_valid_rv, y_pred_valid_xg).round(2)


```





```{python}


xg_accuracy=accuracy_score(y_valid_rv, y_pred_valid_xg).round(2)



```




```{python}



cm_xg_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_xg, labels=[0,1])
df_cm_xg_vl = pd.DataFrame(cm_xg_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_xg_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_xg_vl.flatten()/np.sum(cm_xg_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_xg_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')


plt.title("Confusion Matrix-Extreme Gradient Boost", fontsize=14)



```





```{python}






```



### Feature Importance

```{python}



# let's create a dictionary of features and their importance values
feat_dict_xg= {}
for col, val in sorted(zip(X_train_tr.columns,xg_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict_xg[col]=val


```





```{python}


feat_xg_df = pd.DataFrame({'Feature':feat_dict_xg.keys(),'Importance':feat_dict_xg.values()})



```




```{python}


feat_xg_tp5=feat_xg_df.nlargest(5,"Importance")



```




```{python}



values = feat_xg_tp5.Importance    
idx = feat_xg_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features Extreme Gradient Boost Model')

plt.ylabel("Features", fontsize=9)

plt.tick_params(axis='x', which='major', labelsize=8)

plt.tick_params(axis='y', labelsize=8, labelrotation=45)



```


## Light GBM

<br/><br/>

Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithms grow level-wise (horizontally). It will choose the leaf with max delta (change) loss to grow.

<br/><br/>


```{python}


import lightgbm as lgb



```


<br/><br/>

parameter definition (not previously defined)

        


<br/><br/>


### Training

```{python}


#This parameter defines the number of HP points to be tested
n_HP_points_to_test = 10



```





```{python}



lgb_model = lgb.LGBMClassifier(max_depth=-1, random_state=314, metric='None', n_jobs=-1)



```





```{python}


lgb_search = RandomizedSearchCV(
    estimator=lgb_model, param_distributions=params_lgb, 
    n_iter=n_HP_points_to_test,
    scoring='roc_auc',
    cv=5,
    refit=True,
    random_state=314,
    verbose=True, n_jobs=-1)



```




```{python}



lgb_search.fit(X_train_tr, y_train_rv)



```




```{python}



lgb_clf=lgb_search.best_estimator_




```





```{python}



lgb_clf



```


<br/><br/>

The above display presents the pararmeters chosen for the light gradient boost model

<br/><br/>

### Validation Metrics

```{python}



print('****Light Gradient Boosting Classification Report****')

print(classification_report(y_valid_rv, y_pred_valid_lgb))



```





```{python}


lgb_recall=recall_score(y_valid_rv, y_pred_valid_lgb).round(2)



```





```{python}


lgb_precision=precision_score(y_valid_rv, y_pred_valid_lgb).round(2)




```




```{python}


lgb_f1=f1_score(y_valid_rv, y_pred_valid_lgb).round(2)


```




```{python}



cm_lgb_vl = metrics.confusion_matrix(y_valid_rv, y_pred_valid_lgb, labels=[0,1])
df_cm_lgb_vl = pd.DataFrame(cm_lgb_vl, index=["Actual - No", "Actual - Yes"], columns=["Predicted - No", "Predicted - Yes"])
group_counts = ["{0:0.0f}".format(value) for value in cm_lgb_vl.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm_lgb_vl.flatten()/np.sum(cm_lgb_vl)]
labels = [f"{v1}\n{v2}" for v1, v2 in zip(group_counts, group_percentages)]
labels = np.asarray(labels).reshape(2,2)

plt.figure(figsize=(9,9))
sns.heatmap(df_cm_lgb_vl, annot=labels, fmt='')
plt.ylabel('True label')
plt.xlabel('Predicted label')

plt.title("Confusion Matrix-Light Gradient Boost", fontsize=14)


```



```{python}






```

### Feature Importance


```{python}



feat_dict_lgb= {}
for col, val in sorted(zip(X_train_tr.columns,lgb_clf.feature_importances_),key=lambda x:x[1],reverse=True):
  feat_dict_lgb[col]=val


```



```{python}


feat_lgb_df = pd.DataFrame({'Feature':feat_dict_lgb.keys(),'Importance':feat_dict_lgb.values()})



```



```{python}

feat_lgb_tp5=feat_lgb_df.nlargest(5,"Importance")


```



```{python}


values = feat_lgb_tp5.Importance    
idx = feat_lgb_tp5.Feature
plt.figure(figsize=(12,10))
clrs = ['green' if (x < max(values)) else 'red' for x in values ]
sns.barplot(y=idx,x=values,palette=clrs).set(title='Important features Light Gradient Boost Model')

plt.ylabel("Features", fontsize=9)

plt.tick_params(axis='x', which='major', labelsize=8)

plt.tick_params(axis='y', labelsize=8, labelrotation=45)



```




```{python}







```


## Validation Scores Comparison

```{python}



val_scores=pd.DataFrame({'Model':['Logistic Regression', 'Support Vector Machine', 'Decision Tree', 'Random Forest', 'Ada Boost', 'Gradient Boosting', 'Extreme Gradient Boosting', 'Light GBM'],
'Accuracy':[log_accuracy, svc_accuracy,dt_accuracy, rf_accuracy, adb_accuracy, gb_accuracy, xg_accuracy, lgb_accuracy],
'Recall':[log_recall, svc_recall, dt_recall, rf_recall, adb_recall, gb_recall, xg_recall, lgb_recall],
'Precision':[log_precision, svc_precision, dt_precision, rf_precision, adb_precision, gb_precision, xg_precision, lgb_precision],
'F1':[log_f1, svc_f1, dt_f1, rf_f1, adb_f1, gb_f1, xg_f1, lgb_f1]
})





```





```{python}


r.r_val_scores=val_scores



```



```{r}


val_prec_plt<-ggplot(r_val_scores, aes(x = reorder( Model,-Precision), y=Precision,color=Model)) +
    geom_segment( aes(xend=Model, yend=0)) +
    geom_point( size=5, color="orange") +
  ggtitle("Validation Precision Scores")+
  scale_y_continuous(labels=scales::percent_format())+
  geom_text(aes(label=Precision,
        vjust=.08),size=2.0, color="black")+
    theme(plot.title = element_text(color="blue", size=10, face="bold.italic", hjust=0.5))+
   theme(axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=70, size=8,hjust=1),
        axis.text.y=element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")+
  expand_limits(y = c(0, 0.98))


```



```{r}



val_prec_plt




```




```{r}






val_recall_plt<-ggplot(r_val_scores, aes(x = reorder( Model,-Recall), y=Recall,color=Model)) +
    geom_segment( aes(xend=Model, yend=0)) +
    geom_point( size=5, color="green") +
  ggtitle("Validation Recall Scores")+
  scale_y_continuous(labels=scales::percent_format())+
  geom_text(aes(label=Recall,
        vjust=0.7, hjust=0.3),size=2.0, color="black")+
    theme(plot.title = element_text(color="blue", size=10, face="bold.italic", hjust=0.5))+
   theme(axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.title.x=element_blank(),
        axis.text.y=element_text(size=8,hjust=1),
        axis.text.x=element_blank(),
        axis.ticks = element_blank(),
        legend.position = "none")+
  expand_limits(x = c(0, 0.97))+
  coord_flip()
```



```{r}


val_recall_plt




```




<br/><br/>

From the F1 scores above we find that the logistic regression and decision tree models are below average in predicting the event of interest. The other models have F1 scores that suggest above average capability in predicting the event of interest.

We'll look deeper into the scores by evaluating precision and recall. Precision is the metric we'll focus on as the higher the score the lower false positives are predicted. We do not wish to inaccurately accuse a policy holder of fraud when there is none. On the other hand, a higher recall means lower false negatives (predicting no when it is yes), which we are not concerned with for this analysis.

The extreme gradient boost model has the best balance between precision and recall. It's the only model with a recall score above 0.80. The higher recall score comes at the sacrifice on its precision score which is slightly lower compared to the other models. Interestingly, its important features are different than the other ensemble models.  While all have one significantly important feature, extreme gradient boost has no other features of importance over three percent (*Figure 36*). This differed even from gradient boost (*Figure 34*) which essentially is the same algorithm.  This may be play as part in the ballade of recall and precision scores.

The remaining models have a precision score above .89. These models prove very good at predicting the event of interest (low false positives). A score of 0.90 means only 1 out of 10 policy holders are incorrectly predicted of fraudulent .



# Test Set


<br/><br/>

We are now going to take models with a precision score above .80 and fit to unseen data (test set)

<br/><br/>


## Support Vector Machine

```{python}



y_pred_test_svc=svc_clf.predict(X_test_tr)



```



```{python}






```




```{python}






```




```{python}







```




```{python}






```



```{python}






```








